%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Author: A. Herrera Poyatos
% Tittle: Tests de hipótesis
% Capítulo sobre tests de hipótesis de los apuntes de
% inferencia estadística.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%!TEX root = ../inference.tex
%!TEX language = es

\section{Tests de hipótesis}

    En la Sección \ref{sec:estimacion} se estudió el problema de estimación de parámetros. En este capítulo se estudiará otro problema clásico de la inferencia, los tests de hipótesis.

    \begin{definition}
        Sea $\{f(X|\theta): \theta \in \Theta\}$ una familia de distribuciones. Supongamos que una variable aleatoria sigue una distribución $f(X|\theta)$ con $\theta \in \Theta$. Una hipótesis es una afirmación $\theta \in \Theta_0$, donde $\Theta_0$ es un subconjunto de $\Theta$. Una hipótesis es simple si es de la forma  $\theta \in \{\theta_0\}$ para cierto $\theta_0 \in \Theta$. En tal caso se escribe $\theta = \theta_0$. Si una hipótesis no es simple, entonces decimos que es compuesta. Además, dos hipótesis $\theta \in \Theta_0$ y $\theta \in \Theta_1$ son excluyentes si $\Theta_0$ y $\Theta_1$ son disjuntos.
    \end{definition}

    El objetivo de los tests de hipótesis es, dadas dos hipótesis excluyentes, aceptar una de las dos hipótesis como verdadera tras observar una muestra de tamaño $n$, donde $n$ es un entero positivo fijado de antemano. Esta acción se denomina contrastar dos hipótesis. Generalmente el tratamiento de las dos hipótesis no es simétrico, esto es, una de las dos hipótesis tiene preferencia sobre la otra y solo será rechazada cuando la evidencia en su contra sea muy clara. Esta hipótesis se llama hipótesis nula, mientras que la otra hipótesis se denomina hipótesis alternativa. Las denotamos $H_0: \theta \in \Theta_0$ y $H_1: \theta \in \Theta_1$ respectivamente (recordemos que $\Theta_0 \cap \Theta_1 = \emptyset$).

    El contraste de hipótesis surge de forma natural en multitud de ciencias e ingenierías. Por ejemplo, supongamos que estamos estudiando cómo afecta un determinado medicamento a la presión sanguínea de los pacientes. Queremos corroborar que el medicamento no tiene ningún efecto sobre la presión, hecho que es nuestra hipótesis nula. Sea $\theta$ la variación media de la presión de los pacientes al tomar el medicamento. Esta situación puede representarse como $H_0: \theta = 0$ y $H_1: \theta \ne 0$. Tras tomar muestras en varios pacientes tendremos que decidir cuál de las dos hipótesis aceptamos. Otro ejemplo puede ser el estudio de la proporción media de piezas defectuosas que fabrica una empresa, que denotamos $\theta$. Sea $\theta_0$ el máximo valor aceptable que puede alcanzar esa proporción. Queremos combrobar si la proporción de piezas defectuosas es menor o igual que $\theta_0$, lo que se puede representar mediante las hipótesis $H_0: \theta \le \theta_0$ y $H_1: \theta > \theta_0$. Habitualmente no es factible probar cada una de las piezas y, por tanto, tenemos que hacer inferencia a partir de una muestra. En ambos problemas $\theta$ debe ser el parámetro de una distribución. Por ejemplo, podríamos utilizar una distribución normal de media $\theta$.

    Nótese que en los dos ejemplos anteriores el espacio paramétrico es unión disjunta de $\Theta_0$ y $\Theta_1$. Podemos suponer que siempre se da esta situación. En efecto, el espacio paramétrico siempre se puede restringir a $\Theta_0 \cup \Theta_1$. En este contexto la hipótesis alternativa es la hipótesis complementaria a la hipótesis nula.

    \begin{definition}
        Consideremos una muestra aleatoria simple $\utilde{X} = (X_1, \dots, X_n)$ de $X$. Un test de hipótesis es una regla que permite dividir el espacio muestral de $\utilde{X}$ en dos subconjuntos medibles disjuntos. Estos conjuntos se corresponden con aquellas muestras que aceptan la hipótesis nula como cierta (región de aceptación) y aquellas muestras que rechazan la hipótesis nula y aceptan la hipótesis alternativa (región crítica).
    \end{definition}

    De aquí en adelante supondremos que $n$ está fijado de antemano. Además, supondremos que la región crítica se corresponde con la imagen inversa de un boreliano, esto es, es de la forma $\utilde{X} \in R$ con $R \in \mathcal{B}^n$ ya que el resto de casos no tiene interés práctico. En ocasiones podremos expresar la región crítica en términos de un estadístico $T(X_1, \ldots, X_n)$. Por ejemplo, este estadístico puede ser la media de la muestra y la región crítica aquellas muestras con media mayor que un determinado valor.

    \subsection{Errores de los tests de hipótesis}

        Consideremos un test de hipótesis con región crítica $\utilde{X} \in R$. En dicho test podemos cometer dos tipos de errores:

        \begin{itemize}
            \item \textbf{Error de tipo 1.} Rechazar la hipótesis nula cuando es cierta. Si $\theta \in \Theta_0$, entonces la probabilidad de cometer un error de tipo 1 es $P_\theta(\utilde{X} \in R)$.
            \item \textbf{Error de tipo 2.} Rechazar la hipótesis alternativa cuando es cierta. Si $\theta \in \Theta_0^c$, entonces la probabilidad de cometer un error de tipo 2 es $P_\theta(\utilde{X} \in R^c)$.
        \end{itemize}

        \begin{definition}
            En el contexto actual, se define la función de potencia del test de hipótesis como
            \[\eta(\theta) = P_\theta(\utilde{X} \in R) = \begin{cases} \text{Probabilidad de cometer un error de tipo 1} & \text{ si } \theta \in \Theta_0; \\ \text{1 - Probabilidad de cometer un error de tipo 2} & \text{ si } \theta \in \Theta_0^c. \\ \end{cases}\]
        \end{definition}

        Nuestro objetivo es desarrollar tests de hipótesis tales que la probabilidad de cometer errores de tipo 1 y tipo 2 sea lo más pequeña posible para cualquier valor de $\theta$. Esto es, queremos minimizar $\eta$ en $\Theta_0$ y maximizar $\eta$ en $\Theta_0^c$. Por tanto, la función de potencia ideal es aquella que toma el valor $0$ en $\Theta_0$ y el valor $1$ en $\Theta_0^c$. Esta función solo se obtiene en situaciones triviales. Generalmente obtendremos funciones potencia mucho más complejas. Nótese que si reducimos la región crítica de un test, entonces disminuye la probabilidad de cometer un error de tipo 1 pero aumenta la probabilidad de cometer un error de tipo 2. Consecuentemente, encontrar una región crítica apropiada no es una tarea sencilla.

        Recordemos que la hipótesis nula generalmente tiene preferencia frente a la hipótesis alternativa. Por tanto, el error de tipo 1 es más grave que el error de tipo 2. Para asegurarnos que se respeta esta preferencia podemos buscar tests de hipótesis que garanticen que la probabilidad de cometer un error de tipo 1 es menor que un valor $\alpha$ fijado de antemano.

        \begin{definition}
            Un test de hipótesis es de tamaño $\alpha \in [0,1]$ si $\sup\{\eta(\theta): \theta \in \Theta_0\} = \alpha$.
        \end{definition}

        \begin{definition}
            Un test de hipótesis tiene nivel de significación $\alpha \in [0,1]$ si $\eta(\theta) \le \alpha$ para todo $\theta \in \Theta_0$.
        \end{definition}

        Si nos restringimos a estudiar los tests de tamaño $\alpha$, entonces nuestro objetivo se reduce a buscar entre todos estos tests aquel que minimice la probabilidad de cometer un error de tipo 2. Este hecho queda formalizado en la siguiente definición.

        \begin{definition}
            Considérese un test con función potencia $\eta$. Decimos que es un test uniformemente más potente de tamaño $\alpha$ (resp. de nivel de significación $\alpha$)  si para cualquier otro test de tamaño $\alpha$ (resp. de nivel de significación $\alpha$) con función potencia $\eta'$ se verifica $\eta(\theta) \ge \eta'(\theta)$ para todo $\theta \in \Theta_0^c$. Habitualmente abreviaremos uniformemente más potente por UMP.
        \end{definition}

        En lo que sigue buscaremos obtener los tests UMP de tamaño $\alpha$ en caso de que sea posible. Cabe decir que en la práctica se utilizan valores de $\alpha$ pequeños, como $0.01$, $0.05$ o $0.1$.

    \subsection{Tests de Neyman-Pearson}

        En esta sección estudiamos cuáles son los tests UMP de tamaño y significación $\alpha$ para contrastar dos hipótesis simples. Esta cuestión es resuelta por el Lema de Neyman-Pearson. %Además, utilizaremos estos resultados para obtener tests UMP que contrasten hipótesis compuestas.

        \begin{thm}[Lema de Neyman-Pearson] \label{thm:np:1}
            Supóngase que se desea contrastar dos hipótesis simples, $H_0 : \theta = \theta_0$ y $H_1 : \theta = \theta_1$. Para $k \in \mathbb{R}^+_0$ consideramos la región crítica
            \[C_k = \left\{\utilde{x}: \frac{L(\theta_1;\utilde{x})}{L(\theta_0;\utilde{x})} \ge k\right\}.\]
            Sea $\alpha = P_{\theta_0}(C_k)$. Entonces, el test de región crítica $C_k$ es un test UMP de significación $\alpha$. Además, cualquier test UMP de significación $\alpha$ es de tamaño $\alpha$ y su región crítica $C'$ verifica $\{\utilde{x}: L(\theta_1;\utilde{x}) > k L(\theta_0;\utilde{x})\} \subset C' \subset C_k$ salvo a lo sumo un subconjunto de probabilidad nula.
        \end{thm}
        \begin{proof}
            En primer lugar, nótese que el test dado por $C_k$ es de tamaño $\alpha$ ya que $\sup\{ \eta(\theta) : \theta \in \Theta_0\} = \eta(\theta_0) = \alpha$. Consideremos otro test de significación $\alpha$ cuya función potencia es $\eta'$ y su región crítica es $C'$ y veamos que $\eta'(\theta_1) \le \eta(\theta_1)$. Por distinción de casos es fácil razonar que para cualquier muestra $x$ se verifica
            \begin{equation} \label{eq:np:desigualdad}
                (1_{C_k}(\utilde{x})-1_{C'}(\utilde{x})) (L(\theta_1; \utilde{x}) - k L(\theta_0; \utilde{x})) \ge 0,
            \end{equation}
            donde $1_A$ denota la función indicadora del conjunto $A$. Integrando la desigualdad \eqref{eq:np:desigualdad} obtenemos
            \begin{equation} \label{eq:np:desigualdad:2}
                0 \le \int(1_{C_k}(\utilde{x})-1_{C'}(\utilde{x})) (L(\theta_1; \utilde{x}) - k L(\theta_0; \utilde{x})) \, d\utilde{x} = \eta(\theta_1) - \eta'(\theta_1) - k (\eta(\theta_0) - \eta'(\theta_0)).
            \end{equation}
            Aplicando $\eta(\theta_0) - \eta'(\theta_0) = \alpha  - \eta'(\theta_0) \ge 0$ a \eqref{eq:np:desigualdad:2} deducimos que $0 \le \eta(\theta_1) - \eta'(\theta_1)$ como se quería. Por último, si un test de significación $\alpha$ con función potencia $\eta'$ y región crítica $C'$ es UMP, entonces $\eta(\theta_1) = \eta'(\theta_1)$ y, por tanto, la desigualdad \eqref{eq:np:desigualdad:2} nos indica que $0 \le -k(\eta(\theta_0) - \eta'(\theta_0)) \le 0$, esto es, $\eta'(\theta_0) = \eta(\theta_0) = \alpha$. Puesto que la hipótesis es simple hemos obtenido que el test asociado a $C'$ tiene tamaño $\alpha$. Consecuentemente, en \eqref{eq:np:desigualdad:2} se da la igualdad, cosa que solo puede suceder si la función no negativa a integrar es constantemente 0 salvo en un conjunto de probabiliad nula. La prueba finaliza al darse cuenta de que este hecho equivale a que $\{x: L(\theta_1;\utilde{x}) > k L(\theta_0;\utilde{x})\} \subset C' \subset C_k$ salvo en un subconjunto de probabilidad nula.
        \end{proof}

        En el Teorema \ref{thm:np:1} el valor de $\alpha$ se determina al seleccionar una región crítica $C_k$. La cuestión que uno se hace en este punto es si para cualquier $\alpha \in [0,1]$ existe $k_\alpha$ tal que  $\alpha = P_{\theta_0}(C_{k_\alpha})$. En tal caso podríamos encontrar tests más potentes para cualquier tamaño o significación $\alpha$, que es el valor que nosotros queremos fijar de antemano. La respuesta a esta pregunta es negativa. En efecto, basta con considerar distribuciones discretas ya que para estas distribuciones no podemos asegurar que el valor $\alpha$ sea suma de los valores de la función masa de probabilidad. Por ejemplo, en una distribución binomial $B(x|\theta,n)$ con $\theta$ racional no se puede alcanzar el tamaño $\alpha = 1 / \pi$, que es irracional. No obstante, este problema desaparece en el caso de las distribuciones continuas como muestra el siguiente corolario del Lema de Neyman-Pearson.

        \begin{cor}[Lema de Neyman-Pearson para distribuciones continuas] \label{cor:np:cont}
            Sea $\{f(x|\theta): \theta \in \Theta\}$ una familia de distribuciones continuas para la cual se desean contrastar dos hipótesis simples, $H_0 : \theta = \theta_0$ y $H_1 : \theta = \theta_1$. Supongamos además que las funciones de densidad $f(x|\theta_0)$ y $f(x|\theta_1)$ coinciden a lo sumo en un subconjunto de medida nula y tienen el mismo soporte. Entonces, para cada $\alpha \in [0,1]$ existe $k_\alpha \in \mathbb{R}^+_0$ tal que el test dado por la región crítica
            \[C_{k_\alpha} = \left\{\utilde{x}: \frac{L(\theta_1;\utilde{x})}{L(\theta_0;\utilde{x})} \ge k_\alpha\right\}\]
            es UMP de tamaño $\alpha$.
        \end{cor}
        \begin{proof}
            Gracias al Lema de Neyman-Pearson la prueba se reduce a comprobar que existe $k_\alpha$ tal que $\alpha = P_{\theta_0}(C_k)$. En efecto, la continuidad de las funciones de densidad nos asegura que la función $f(x| \theta_1) / f(x| \theta_0)$ es continua. Definimos $\varphi: [0,1] \to [0,1]$ dada por
            \[\varphi(k) = P_{\theta_0}\left[\frac{L(\theta_1;\utilde{X})}{L(\theta_0;\utilde{X})} \ge k\right].\]
            Esta función es continua y verifica $\varphi(0) = 1$ y $\lim_{k \to \infty} \varphi(k) = 0$. El resultado es consecuencia del teorema del valor intermedio aplicado a $\varphi$.
        \end{proof}

        Los tests proporcionados por el lema de Neyman-Pearson se conocen como tests de Neyman-Pearson. Los estadísticos suficientes son de especial ayuda al aplicar este tipo de tests. En efecto, si $T$ es un estadístico suficiente y $h(t | \theta)$ es su función de densidad, entonces
        \[\frac{L(\theta_1;\utilde{x})}{L(\theta_0;\utilde{x})} = \frac{h(T(\utilde{x}) | \theta_1)}{h(T(\utilde{x}) | \theta_0)}.\]
        Por tanto, podemos calcular la región crítica del test simplemente conociendo el estadístico suficiente y su distribución. Esta observación hace que los cálculos sean más sencillos.

        A continuación estudiamos varios ejemplos de los tests de Neyman-Pearson.

        \begin{ex}[Distribución normal de varianza conocida] \label{ex:np:1}
            Consideramos una variable aleatoria $X \sim N(\mu, \sigma^2)$, donde $\sigma^2$ es conocido. Vamos a contrastar las hipótesis $H_0 : \mu = \mu_0$ y $H_1 : \mu = \mu_1$ con $\mu_1 > \mu_0$. A priori uno intuye que habrá que rechazar $H_0$ cuando la media muestral $\overline{x}$ se encuentre más cerca de $\mu_1$ que de $\mu_0$. Veamos que el test de Neyman-Pearson sigue esta intuición.

            Recordemos que en el Ejemplo \ref{ex:sufi:normal:media} se demostró que la media muestral $\overline{X}$ es un estadístico suficiente de cualquier distribución normal de varianza conocida. Además, obtuvimos que $\overline{X} \sim N(\mu, \sigma^2 / n)$. Esta observación permite calcular la región crítica del test de Neyman-Pearson con facilidad. En efecto,
            \[\frac{L(\mu_1;\utilde{x})}{L(\mu_0;\utilde{x})} = \exp\left(-\frac{(\overline{x} - \mu_1)^2 - (\overline{x} - \mu_0)^2}{2\sigma^2/n}\right) = \exp\left(-\frac{\mu_1^2 - \mu_0^2 + 2\overline{x}(\mu_0-\mu_1)}{2\sigma^2/n}\right).\]
            Como consecuencia, obtenemos que $L(\mu_1;\utilde{x}) / L(\mu_0;\utilde{x}) \ge k$ si, y solo si,
            \[\overline{x} \ge \frac{\mu_0+\mu_1}{2} + \frac{\sigma^2 \log k}{n(\mu_1 - \mu_0)} = A_k.\]
            Dado $\alpha \in [0,1]$ buscamos $k_\alpha \in \mathbb{R}^+_0$ tal que $P_{\mu_0}(\overline{X} \ge A_{k_\alpha}) = \alpha$. Esta ecuación podemos resolverla para valores concretos de $\alpha$. Bajo $H_0$ tenemos $\overline{X} \sim N(\mu_0, \sigma^2 / n)$ y, por tanto, sabemos que $A_{k_\alpha} = \mu_0 + z_\alpha \sigma / \sqrt{n}$, donde $z_\alpha$ verifica $F(z_\alpha) = 1 - \alpha$ para la función de distribución $F$ de $N(0,1)$. Los valores $z_\alpha$ que más se utilizan aparecen habitualmente en las tablas de la distribución normal. También se pueden calcular mediante un ordenador.

            Por ejemplo, supongamos que $\mu_0 = 0$, $\mu_1 = 1$, $\sigma = 1$ y $\alpha = 0.05$. Además, consideremos que se toman muestras de tamaño $n = 100$. En este caso tenemos que $z_\alpha = 1.645$. Por tanto, $A_{k_\alpha} = z_\alpha / \sqrt{100} = 0.1645$. Esto es, rechazaremos la hipótesis $H_0$ si $\overline{x} \ge 0.1645$.

            Habitualmente se denota $Z = \sqrt{n} (\overline{X} - \mu_0) / \sigma$. Bajo $H_0$ esta variable sigue la distribución $N(0,1)$. El test que hemos obtenido nos dice que rechazaremos $H_0$ cuando $z = \sqrt{n} (\overline{x} - \mu_0) / \sigma \ge z_\alpha$. Debido al uso del estadístico $Z$ este test se conoce comúnmente como \emph{test Z}. En este caso, $z = 10 \overline{x}$.
        \end{ex}


    \subsection{Descripción de un test mediante p-valores}

        En el Ejemplo \ref{ex:np:1} hemos realizado el test para un determinado valor de $\alpha$, obteniendo una desigualdad que nos indica si aceptamos o rechazamos la hipótesis nula en función de la muestra. No obstante, algunas muestras tienen más evidencia en su contra que otras. %Si reducimos el valor de significación del tests, entonces aceptaremos muestras con mayor evidencia en contra de la hipótesis nula.
        Por tanto, a la hora de realizar un test es conveniente obtener un indicador de la evidencia que tiene una muestra en contra de la hipótesis nula. En este punto entran en juego los p-valores.

        \begin{definition}
            En un contraste de hipótesis un p-valor $p(\utilde{X})$ es un estadístico que verifica $0 \le p(\utilde{x}) \le 1$ para cualquier punto $\utilde{x}$ del espacio muestral. Un p-valor es válido si para cualquier $\theta \in \Theta_0$ y $\alpha \in [0,1]$ se tiene $P_\theta(p(\utilde{X}) \le \alpha) \le \alpha.$
        \end{definition}

        A patir de un p-valor válido $p(\utilde{X})$ podemos construir un test con nivel de significación $\alpha$ para cualquier $\alpha \in [0,1]$. Este test rechaza $H_0$ si y solo si $p(\utilde{X}) \le \alpha$.

        \begin{definition}
            Supongamos que un test de hipótesis se puede formular para cualquier nivel de significación $\alpha$. Sea $p(\utilde{X})$ un p-valor válido. Decimos que el test es descrito por el p-valor $p(\utilde{X})$ si , y solo si, la región crítica del test con significación $\alpha$ es $\left[p(\utilde{X}) \le \alpha\right]$.
        \end{definition}

        En un test descrito por un p-valor podemos elegir el valor de $\alpha$ que consideremos apropiado y simplemente compararlo con $p(\utilde{x})$ para saber cuál es el resultado del test. Nótese que valores pequeños de $p(\utilde{X})$ dan evidencia de que $H_1$ es cierta y, por tanto, el p-valor mide la evidencia a favor de la hipótesis nula. Por tanto, realizar un test descrito por un p-valor es más informativo que simplemente elegir entre ``aceptar $H_0$'' o ``rechazar $H_0$''.

        El siguiente resultado nos informa sobre cómo construir un p-valor válido.

        \begin{thm}[{\cite[Teorema 8.3.27]{casella}}] \label{thm:p-valor}
            Consideramos el contraste de hipótesis dado por $H_0: \theta \in \Theta_0$ y $H_1: \theta \in \Theta_1$. Sea $W(\utilde{X})$ un estadístico. Entonces, la función
            \[p(\utilde{x}) = \sup\{P_\theta(W(\utilde{X}) \ge W(\utilde{x})): \theta \in \Theta_0\}\]
            es un p-valor válido.
        \end{thm}
        %\begin{proof}
        %    La prueba se reduce a aplicar la definición de p-valor. En efecto, el p-valor es igual a
        %    \[\sup\{\alpha: W(\utilde{x}) \ge c_\alpha\} = \sup\{P_\theta(W(\utilde{X}) \ge W(\utilde{x})): \theta \in \Theta_0\}. \qedhere\]
        %\end{proof}

        La pregunta que uno se hace en este punto es, dado un test que se puede formular para cualquier valor de $\alpha$, cómo obtener un p-valor que lo describa. Esto no va a ser siempre factible ya que habrá regiones críticas que no sepamos o no se puedan escribir de la forma $\left[p(\utilde{X}) \le \alpha\right]$. No obstante, en múltiples casos prácticos sí es posible como muestra el siguiente resultado. %El siguiente resultado proporciona el p-valor de los tests de Neyman-Pearson.

        \begin{thm} \label{thm:p-valor:2}
            Sea $W(\utilde{X})$ un estadístico. Supongamos que para cada $\alpha \in [0,1]$ disponemos de un test de hipótesis de tamaño $\alpha$ cuya región crítica es $W(\utilde{X}) \ge k_\alpha$ para cierta constante $k_\alpha$. Entonces, el test se puede describir a partir del p-valor
           \[p(\utilde{x}) = \sup\{P_{\theta}\left(W(\utilde{X}) \ge W(\utilde{x})\right): \theta \in \Theta_0\}.\]
       \end{thm}
        \begin{proof}
            Nótese que $p(\utilde{x})$ es un p-valor gracias al Teorema \ref{thm:p-valor}. Sea $\alpha \in [0,1]$ y sea $k_\alpha$ como en el enunciado. Tenemos que $p(\utilde{x}) = \sup\{P_{\theta}\left(W(\utilde{X}) \ge k_\alpha)\right): \theta \in \Theta_0\} \le \alpha$ si, y solo si, para cada $\theta \in \Theta_0$ se tiene
            \[P_{\theta}\left(W(\utilde{X}) \ge W(\utilde{x})\right) \le P_{\theta}\left(W(\utilde{X}) \ge k_\alpha\right) ,\]
            esto es, $W(\utilde{x}) \ge k_\alpha$.
        \end{proof}

        \begin{cor} \label{cor:p-valor:np}
            Consideremos un test de Neyman-Pearson con función de densidad continua. Entonces, el test se puede describir a partir del p-valor
           \[p(\utilde{x}) = P_{\theta_0}\left(\frac{L(\theta_1;\utilde{X})}{L(\theta_0;\utilde{X})} \ge \frac{L(\theta_1;\utilde{x})}{L(\theta_0;\utilde{x})}\right).  \]
       \end{cor}

        El p-valor que hemos construido en el Teorema \ref{thm:p-valor:2} otorga a una muestra $\utilde{x}$ evidencia contra la hipótesis nula cuando el valor de $W(\utilde{x})$ es muy alto. El valor obtenido al aplicar este p-valor a una muestra puede interpretarse como la probabilidad de observar una muestra que sea tan poco favorable a la hipótesis nula como ella. Esto es lo que sucede en el Corolario \ref{cor:p-valor:np}, donde el p-valor es efectivamente una probabilidad. Algunos usuarios de los tests de hipótesis interpretan el p-valor como la probabilidad de que la hipótesis nula sea falsa para la muestra obtenida. Esto es erróneo y en ningún momento deberíamos utilizar esta interpretación. En la Sección \ref{sec:bayes} estudiaremos tests de hipótesis desde el punto de vista bayesiano. En estos tests sí obtendremos probabilidades de que la hipótesis nula sea cierta o no.

        \begin{ex}[Continuación del Ejemplo \ref{ex:np:1}]
            Recordemos que obtuvimos que la región crítica del test Z está determinada por $\overline{X} \ge A_{k_\alpha}$. Por tanto, el Teorema \ref{thm:p-valor:2} nos dice que el p-valor del test es $p(\utilde{x}) = P_{\mu_0}(\overline{X} \ge \overline{x})$. También probamos que podemos describir la región crítica en términos de la variable $Z$. De hecho, $P_{\mu_0}(\overline{X} \ge \overline{x}) = P_{\mu_0}(Z \ge \sqrt{n}(\overline{x} - \mu_0)/\sigma)$, lo que nos permite calcular los p-valores gracias a que $Z \sim N(0,1)$.

            Veamos un ejemplo numérico. Los datos osn los mismos que los del Ejemplo \ref{ex:np:1}. Supongamos que tras observar una muestra con $n = 100$ hemos obtenido $\overline{x} = 0.2$. Entonces, $p(\utilde{x}) = P_{\mu_0}(Z \ge 2) = 1 - F(2) = 1 - 0.9772 = 0.0328$, donde $F$ es la función de distribución de N(0,1). Por tanto, rechazaremos la hipótesis nula para cualquier nivel de significación mayor que $0.0328$.
        \end{ex}

    \subsection{Tests de la razón de verosimilitud}

        En esta sección estudiamos los tests de la razón de la verosimilitud, que están íntimamente ligados con los estimadores máximo verosímiles. Además, veremos que en el caso de hipótesis simples estos tests coinciden con el test de Neyman-Pearson.

        \begin{definition}
            Considérese una hipótesis $H : \theta \in \Theta_0$. El ratio de verosimilitud de $H$ para la muestra $\utilde{x}$ es
            \[\lambda(\utilde{x}) = \frac{\sup\{L(\theta;\utilde{x}): \theta \in \Theta_0\}}{\sup\{L(\theta;\utilde{x}): \theta \in \Theta\}}. \]
            Fijado un contraste de hipótesis, $H_0 : \theta \in \Theta_0$ y $H_1 : \theta \in \Theta_1$, un test de la razón de verosimilitud es cualquier test cuya región crítica sea de la forma $\{\utilde{x} : \lambda(\utilde{x}) \le c\}$, donde $0 \le c \le 1$.
        \end{definition}

        Para motivar la definición de este tipo de test, supongamos que estamos trabajando con distribuciones discretas. En tal caso, tanto el numerador como el denominador de $\lambda(\utilde{x})$ se corresponden con la máxima probabilidad posible de la muestra $\utilde{x}$ si variamos $\theta$ en $\Theta_0$ y $\Theta$ respectivamente. Si el cociente de ambos valores es pequeño ($\lambda(\utilde{x}) \le c$), entonces es razonable rechazar la hipótesis nula puesto que hay elementos de $\Theta_0^c$ para los cuales la muestra es más probable.

        Supongamos ahora que existen los estimadores máximo verosímiles de $\theta_0$ en $\Theta_0$ y $\Theta$. Denotamos a estos estimadores $\hat{\theta_0}(x)$ y $\hat{\theta}(x)$ respectivamente. Entonces, $\lambda(\utilde{x}) = L(\hat{\theta_0}(x);\utilde{x}) / L(\hat{\theta}(x);\utilde{x})$. El test de la razón de verosimilitud nos dice que rechazaremos la hipótesis nula cuando $\hat{\theta}(x)$  tenga una credibilidad considerablemente mayor que la de $\hat{\theta_0}(x)$, esto es, $\hat{\theta}(x)$ es mucho mejor estimador. Como caso particular, si a partir de una muestra $x$ hemos realizado una estimación de $\theta$ mediante un estimador máximo verosímil $\hat{\theta}$, entonces todo test de la razón de verosimilitud acepta la hipótesis $H_0: \theta = \hat{\theta}$ para la muestra $x$. Esto es, la filosofía de los tests de la razón de verosimilitud es coherente con la filosofía de los estimadores máximo verosímiles.

        \begin{ex}[\textbf{Test t de Student} -- distribución normal con $\mu$ y $\sigma^2$ desconocidos]
            Consideramos una variable $X \sim N(\mu, \sigma^2)$ con $\mu$ y $\sigma^2$ desconocidos y buscamos contrastar las hipótesis $H_0: \mu = \mu_0$ y $H_1: \mu \ne \mu_0$ aplicando el test de la razón de verosimilitud. Esto es, el parámetro es $\theta = (\mu, \sigma^2)$, $\Theta = \mathbb{R}\times\mathbb{R}^+$ y $\Theta_0 = \{0\}\times\mathbb{R}^+$. Este test se denomina test t de Student.

            Tenemos que calcular $\lambda(\utilde{x})$ para cualquier muestra $\utilde{x}$. Recordemos que en el Ejemplo \ref{ex:sufi:normal:2} se demostró que la función de verosimilitud en este contexto responde a
            \[L(\mu, \sigma; \utilde{x}) = (2 \pi \sigma^2)^{-n/2} \exp\left(\frac{-1}{2 \sigma^2} \sum_{i = 1}^n (x_i - \mu)^2\right) = (2 \pi \sigma^2)^{-n/2} \exp\left(\frac{-1}{2 \sigma^2} ((n-1)S^2 + n(\overline{x}-\mu))\right),\]
            donde $S^2$ es la varianza muestral. En primer lugar calculamos
            \[\lambda_0(\utilde{x}) = \sup\{L(\theta;\utilde{x}): \theta \in \Theta_0\} = \sup\{L(\mu_0, \sigma^2;\utilde{x}): \sigma^2 \in \mathbb{R}^+\}.\]
            Es fácil ver que el estimador máximo verosímil de la varianza de una normal con media conocida $\mu_0$ es $\hat{\sigma}_0^2(\utilde{x}) = \frac{1}{n}\sum_{i = 1}^n (x_i - \mu_0)^2$. Por tanto, obtenemos
            \[\lambda_0(\utilde{x}) = L(\mu_0, \hat{\sigma}_0^2; \utilde{x}) = (2 \pi \hat{\sigma}_0^2)^{-n/2} \exp(-n / 2).\]
            Por otro lado, en el caso de una normal con media y varianza desconocida el estimador máximo verosímil $\hat{\theta} = (\hat{\mu}, \hat{\sigma}^2)$ viene dado por $\hat{\mu}(\utilde{x}) = \overline{x}$ y $\hat{\sigma}^2(\utilde{x}) = \frac{1}{n}\sum_{i = 1}^n (x_i - \overline{x})^2 = (n-1)S^2/n$. Por tanto, obtenemos
            \[\lambda_1(\utilde{x}) = \sup\{L(\theta;\utilde{x}): \theta \in \Theta\} = (2 \pi \hat{\sigma}^2)^{-n/2} \exp(-n / 2).\]
            El ratio de verosimilitud de $H_0$ es
            \begin{equation} \label{eq:lrt:normal}
            \begin{split}
            \lambda(\utilde{x}) = \frac{\lambda_0(\utilde{x})}{\lambda_1(\utilde{x})} = \left(\frac{\hat{\sigma}_0^2}{\hat{\sigma}^2}\right)^{-n/2} = \left(\frac{\frac{1}{n}\sum_{i = 1}^n (x_i - \mu_0)^2}{(n-1)S^2/n}\right)^{-n/2} = \\ \left(\frac{\sum_{i = 1}^n (x_i - \overline{x})^2 + n(\overline{x} - \mu_0)^2}{(n-1)S^2}\right)^{-n/2} =
            \left(1 + \frac{t^2}{n-1}\right)^{-n/2},
            \end{split}
            \end{equation}
            donde $t = \sqrt{n}(\overline{x} - \mu_0) / S$. Recordemos en este punto que bajo $H_0$ la variable $T = \sqrt{n}(\overline{X} - \mu_0) / S$ sigue una distribución t de Student con $n-1$ grados de libertad. A partir de \eqref{eq:lrt:normal} deducimos que $\lambda$ es decreciente respecto de $|t|$. Por tanto, el test de la razón de verosimilitud tiene como región crítica
            \[\left[\frac{\sqrt{n}|\overline{X} - \mu_0|}{S} \ge b\right]\]
            para cierta constante $b \in [0,1]$. La región crítica no depende de $\sigma^2$. Por tanto, tenemos que $\eta(\theta) = P_{\mu_0}(|T| \ge b) = 2 P_{\mu_0}(T \le -b)$ para todo $\theta \in \Theta_0$. Consecuentemente el tamaño del test es $\sup\{\eta(\theta): \theta \in \Theta_0\} = 2P_{\mu_0}(T \le -b)$. En la práctica determinamos $b$ de manera que el tamaño del test sea igual a $\alpha$. Para ello utilizamos la función de distribución de la distribución t de Student con $n-1$ grados de libertad.
            % Ejemplo de Ingeniería de Servidores
            Por último, cabe hablar del p-valor del test t de Student. Utilizando el Teorema \ref{thm:p-valor:2} obtenemos que el p-valor viene dado por
            \[p(\utilde{x}) = 2P_{\mu_0}\left[\frac{\sqrt{n}(\overline{X} - \mu_0)}{S} \le -t\right]. \qedhere\]
        \end{ex}

        La principal propiedad del test de la razón de verosimilitud y que justifica su extendido uso es el siguiente resultado, cuya demostración puede verse en \cite{garthwaite}, página 84.

        \begin{thm}
            Consideramos el contraste de hipótesis dado por $H_0: \theta = \theta_0$ y $H_1: \theta \ne \theta_0$. Si existe un test UMP para contrastar ambas hipótesis, entonces el test de la razón de verosimilitud coincide con el test UMP.
        \end{thm}

        Como consecuencia, en el caso de hipótesis simples el test de la razón de verosimilitud y el test de Neyman-Pearson son equivalentes. En lo que sigue estudiamos las propiedades asintóticas del test de la razón de verosimilitud. Una de las propiedades más destacadas es la siguiente.

        \begin{thm}
            Supongamos que se cumplen las hipótesis de regularidad de Cramer-Rao y que, además, la verosimilitud es de clase 2. Deseamos contrastar dos hipótesis $H_0: \theta \in \Theta_0$ y $H_1: \theta \in \Theta_1$. Bajo la hipotesis nula se tiene que
            \[-2 \log \lambda(\utilde{X}) \to \chi_d^2\]
            cuando el tamaño de la muestra diverge, donde $d = dim(\Theta) - dim(\Theta_0)$ ($dim(\Omega)$ indica el número de variables no relacionadas entre sí de $\Omega \subset \mathbb{R}^n$).
        \end{thm}

        \begin{proof}
            Realizamos la demostración para el caso $H_0: \theta = \theta_0$ y $H_1: \theta \ne \theta_0$. En otro caso puede consultarse Cristóbal (1992), Teorema 1.2, página 596. En este contexto $d = 1$. Tenemos que
            \[\mathcal{Q}_n = -2 \log \lambda(\utilde{X}) = 2 (\log L(\hat{\theta}_n; \utilde{X}) - \log L(\theta_0; \utilde{X})),\]
            donde $\hat{\theta}_n = \hat{\theta}_n(\utilde{X})$ es el estimador máximo verosímil de $\theta$, que existe gracias al Teorema \ref{thm:emv:consistencia}. Desarrollamos el término $\log L(\theta_0; \utilde{X})$ al rededor de $\hat{\theta}_n$ mediante el teorema de Taylor, obteniendo
            \[\log L(\theta_0; \utilde{X}) = \log L(\hat{\theta}_n; \utilde{X}) + \frac{\partial \log L(\hat{\theta}_n; \utilde{X})}{\partial \theta}(\theta_0 -  \hat{\theta}_n) + \frac{1}{2} \frac{\partial^2 \log L(\theta'; \utilde{X})}{\partial \theta^2}(\theta_0 -  \hat{\theta}_n)^2,\]
            donde $\theta'$ se encuentra entre $\theta_0$ y $\hat{\theta}_n$. El estimador máximo verosímil del Teorema \ref{thm:emv:consistencia} maximiza localmente la verosimilitud. Por tanto,
            \[0 = \frac{\partial \log L(\hat{\theta}_n; \utilde{X})}{\partial \theta}.\]
            En consecuencia podemos escribir $\mathcal{Q}_n$ como sigue
            \[\mathcal{Q}_n = - \frac{\partial^2 \log L(\theta'; \utilde{X})}{\partial \theta^2}(\theta_0 -  \hat{\theta}_n)^2 = - \frac{1}{n} \frac{\partial^2 \log L(\theta'; \utilde{X})}{\partial \theta^2}(\sqrt{n}(\theta_0 -  \hat{\theta}_n))^2.\]
            El Teorema \ref{thm:emv:an} asegura que bajo las hipótesis de Cramer-Rao el estimador máximo verosímil es asintóticamente normal, esto es, $\sqrt{n}(\theta_0 -  \hat{\theta}_n)$ converge en ley a $N(0, 1/\mathcal{I}(\theta_0))$. Por otro lado, gracias a la ley fuerte de los grandes números tenemos que
            \[- \frac{1}{n} \frac{\partial^2 \log L(\theta_0; \utilde{X})}{\partial \theta^2} = \frac{1}{n} \sum_{i = 1}^n - \frac{\partial^2 \log L(\theta_0; X_i)}{\partial \theta^2}\]
            converge casi seguramente a la esperanza de una de estas variables, que es $\mathcal{I}(\theta_0)$ por el Lema \ref{lem:fisher:2dev}. Este hecho junto con la consistencia del estimador máximo verosímil nos asegura que
            \[- \frac{1}{n} \frac{\partial^2 \log L(\theta'; \utilde{X})}{\partial \theta^2} = - \frac{1}{n} \frac{\partial^2 \log L(\theta_0; \utilde{X})}{\partial \theta^2} + \frac{1}{n}\left(\frac{\partial^2 \log L(\theta_0; \utilde{X})}{\partial \theta^2} -  \frac{\partial^2 \log L(\theta'; \utilde{X})}{\partial \theta^2}\right)\]
            converge casiseguramente a $\mathcal{I}(\theta_0)$. Podemos pues completar el estudio asintótico de $\sqrt{\mathcal{Q}_n}$, que converge en ley a $N(0, 1)$. Por último, la Proposición \ref{prop:normal-square} concluye que $\mathcal{Q}_n$ converge en ley a $\chi_1^2$.
        \end{proof}

        En muchas ocasiones es difícil calcular la región crítica del test de la razón de verosimilitud. En tales caso, si el tamaño de la muestra es muy grande, entonces podemos suponer que bajo la hipótesis nula $-2 \log \lambda(\utilde{X})$ sigue una distribución $\chi_d^2$ y proceder con el test de hipótesis como es habitual.

        La siguiente definición extiende la definición de consistencia a los tests de hipótesis.

        \begin{definition}
            Un test de hipótesis se dice consistente si verifica
            \begin{enumerate}
                \item $\lim_{n \to \infty} P_{\theta}(W_\alpha) = 0$ para todo $\theta \in \Theta_0$;
                \item $\lim_{n \to \infty} P_{\theta}(W_\alpha) = 1$ para todo $\theta \in \Theta_1$.
            \end{enumerate}
        \end{definition}

        El test de la razón de verosimilitud no es consistente ya que $P_{\theta}(W_\alpha) = \alpha$ para todo $n \in \mathbb{N}$. Este es el principal problema de los tests de hipótesis clásicos. El hecho de seleccionar un tamaño del test implica que el comportamiento asintótico no es el deseable. En un futuro veremos que en la mayoría de los casos la inferencia bayesiana resuelve este problema. No obstante, bajo determinadas hipótesis el test de la razón de verosimilitud sí verifica el apartado b) de la definición previa. El siguiente resultado es un ejemplo de este hecho.

        \begin{prop}
            Supongamos que se cumplen las hipótesis de regularidad de Cramer-Rao y que, además, la verosimilitud es de clase 2. Deseamos contrastar dos hipótesis $H_0: \theta = \theta_0$ y $H_1: \theta \ne \theta_0$ mediante un test de la razón de verosimilitud cuya región crítica para significación $\alpha$ es $W_\alpha$. Entonces, $\lim_{n \to \infty} P_{\theta}(W_\alpha) = 1$ para todo $\theta \in \Theta_1$.
        \end{prop}

        \begin{proof}
            Supongamos que $\theta_1 \ne \theta_0$ es el verdadero valor del parámetro $\theta$. Denotemos por $\lambda(\utilde{x})$ al ratio de verosimilitud asociado a este test. Sea $\hat{\theta}_n$ el estimador máximo verosímil dado por el Teorema \ref{thm:emv:consistencia}.  Consideremos también el test $H'_0: \theta = \theta_1$ y $H'_1: \theta \ne \theta_1$ y denotemos por $\lambda'(\utilde{X})$ a su ratio de verosimilitud. Tenemos que
            \[\lambda(\utilde{X}) = \frac{L(\theta_0; \utilde{X})}{L(\hat{\theta}_n; \utilde{X})} = \frac{L(\theta_1; \utilde{X})}{L(\hat{\theta}_n; \utilde{X})} \frac{L(\theta_0; \utilde{X})}{L(\theta_1; \utilde{X})} = \lambda'(\utilde{X}) \frac{L(\theta_0; \utilde{X})}{L(\theta_1; \utilde{X})}.\]
            Por consiguiente podemos escribir $- 2 \log \lambda(\utilde{X}) = - 2 \log \lambda'(\utilde{X}) - 2 \log L(\theta_0; \utilde{X}) + 2 \log L(\theta_1; \utilde{X})$.  Además,  la ley fuerte de los grandes números nos asegura que la variable aletoria
            \[\frac{1}{n} \left(- 2\log L(\theta_0; \utilde{X}) + 2 \log L(\theta_1; \utilde{X})\right) = - 2 \frac{1}{n} \sum_{i = 1}^n \log L(\theta_0; X_i) + 2 \frac{1}{n} \sum_{i = 1}^n \log L(\theta_1; X_i)\]
            converge casi seguramente a $2(E_{\theta_1}(\log L(\theta_1; X_1)) - E_{\theta_1}(\log L(\theta_0; X_1)))$, que es un valor positivo gracias a la Proposición \ref{prop:desigualdad}. Recordando que $-2 \log \lambda'(\utilde{X})$ converge en ley a $\chi_1^2$, obtenemos que $- 2 \log \lambda(\utilde{X}) \to +\infty$ casi seguramente. En particular, hemos deducido que
            \[P_{\theta_1}(W_\alpha) = P_{\theta_1}[- \log \lambda(\utilde{X}) \ge k_\alpha] \to 1. \qedhere\]
        \end{proof}
