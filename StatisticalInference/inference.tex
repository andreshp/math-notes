%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Plantilla básica de Latex en Español.
%
% Autor: Andrés Herrera Poyatos (https://github.com/andreshp)
%
% Es una plantilla básica para redactar documentos. Utiliza el paquete fancyhdr para darle un
% estilo moderno pero serio.
%
% La plantilla se encuentra adaptada al español.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%-----------------------------------------------------------------------------------------------------
%	TEX OPTIONS
%-----------------------------------------------------------------------------------------------------

% !TEX program = pdflatex
% !TEX option = -shell-escape
% !TEX language = es

%-----------------------------------------------------------------------------------------------------
%	INCLUSIÓN DE PAQUETES BÁSICOS
%-----------------------------------------------------------------------------------------------------

\documentclass{article}

%-----------------------------------------------------------------------------------------------------
%	SELECCIÓN DEL LENGUAJE
%-----------------------------------------------------------------------------------------------------

% Paquetes para adaptar Látex al Español:
\usepackage[spanish,es-noquoting, es-tabla, es-lcroman]{babel} % Cambia
\usepackage[utf8]{inputenc}                                    % Permite los acentos.
\selectlanguage{spanish}                                       % Selecciono como lenguaje el Español.

%-----------------------------------------------------------------------------------------------------
%	SELECCIÓN DE LA FUENTE
%-----------------------------------------------------------------------------------------------------

% Fuente utilizada.
\usepackage{courier}                    % Fuente Courier.
\usepackage{microtype}                  % Mejora la letra final de cara al lector.


%-----------------------------------------------------------------------------------------------------
%	LICENCIA
%-----------------------------------------------------------------------------------------------------

\usepackage[
    type={CC},
    modifier={by},
    version={4.0},
]{doclicense}

%-----------------------------------------------------------------------------------------------------
%	ESTILO DE PÁGINA
%-----------------------------------------------------------------------------------------------------

% Estílo de capítulos (usar book en lugar de article).
%\usepackage[Lenny]{fncychap}

% Paquetes para el diseño de página:
\usepackage{fancyhdr}               % Utilizado para hacer títulos propios.
\usepackage{lastpage}               % Referencia a la última página. Utilizado para el pie de página.
\usepackage{extramarks}             % Marcas extras. Utilizado en pie de página y cabecera.
\usepackage[parfill]{parskip}       % Crea una nueva línea entre párrafos.
\usepackage{geometry}               % Asigna la "geometría" de las páginas.

% Se elige el estilo fancy y márgenes de 3 centímetros.
\pagestyle{fancy}
\geometry{left=3cm,right=3cm,top=3cm,bottom=3cm,headheight=1cm,headsep=0.5cm} % Márgenes y cabecera.
% Se limpia la cabecera y el pie de página para poder rehacerlos luego.
%\fancyhfb{}

% Espacios en el documento:
\linespread{1.1}                        % Espacio entre líneas.
\setlength\parindent{0pt}               % Selecciona la indentación para cada inicio de párrafo.

% Cabecera del documento. Se ajusta la línea de la cabecera.
\renewcommand\headrule{
	\begin{minipage}{1\textwidth}
	    \hrule width \hsize
	\end{minipage}
}

% Texto de la cabecera:
\lhead{A. Herrera, N. Rodríguez, J.L. Suárez}                          % Parte izquierda.
\chead{}                                    % Centro.
\rhead{\subject \ - \doctitle}              % Parte derecha.

% Pie de página del documento. Se ajusta la línea del pie de página.
\renewcommand\footrule{
\begin{minipage}{1\textwidth}
    \hrule width \hsize
\end{minipage}\par
}

\lfoot{}                                                 % Parte izquierda.
\cfoot{}                                                 % Centro.
\rfoot{Página\ \thepage\ de\ \protect\pageref{LastPage}} % Parte derecha.

%-----------------------------------------------------------------------------------------------------
%	Secciones
%-----------------------------------------------------------------------------------------------------

\newcommand{\importsection}[1]{\input{./Sections/#1}}           % Include sections from sections directory.

%-----------------------------------------------------------------------------------------------------
%	PORTADA
%-----------------------------------------------------------------------------------------------------

% Elija uno de los siguientes formatos.
% No olvide incluir los archivos .sty asociados en el directorio del documento.
\usepackage{title1}
%\usepackage{title2}
%\usepackage{title3}

%----------------------------------------------------------------------------------------
%   MATEMÁTICAS
%----------------------------------------------------------------------------------------

\usepackage{mathematics} % Llama al paquete de matemáticas que se adjunta con la plantilla

% Pone una tilde debajo de la palabra.
\def\utilde#1{\mathord{\vtop{\ialign{##\crcr
$\hfil\displaystyle{#1}\hfil$\crcr\noalign{\kern1pt\nointerlineskip}
$\hfil\widetilde{}\hfil$\crcr\noalign{\kern-5pt\nointerlineskip}}}}}

\usepackage{comment}

%----------------------------------------------------------------------------------------
%   GRÁFICOS
%----------------------------------------------------------------------------------------

\usepackage{pgfplots}
\usepackage{tikz}
% Load the library (Descomentar para SOs distintos de Windows)
%\usetikzlibrary{external}
% Enable the library !!!>>> MUST be in the preamble <<<!!!!
%\tikzexternalize
\pgfplotsset{compat=newest}

\usepackage{float}

%----------------------------------------------------------------------------------------
%   ENLACES
%----------------------------------------------------------------------------------------
\usepackage{hyperref}
\hypersetup{
    colorlinks   = true,   % Quita las cajas y añade un color al texto.
    % Tipos de enlaces cuyo color se puede configurar:
    linkcolor    = [rgb]{0,0.2,0.5},        % Por defecto red
    anchorcolor  = gray,        % Por defecto black
    citecolor    = magenta,     % Por defecto green
    filecolor    = red,         % Por defecto cyan
    menucolor    = green,       % Por defecto red
    runcolor     = red,         % Por defecto cyan
    urlcolor     = cyan        % Por defecto magenta
}

%-----------------------------------------------------------------------------------------------------
%	TÍTULO, AUTOR Y OTROS DATOS DEL DOCUMENTO
%-----------------------------------------------------------------------------------------------------

% Título del documento.
\newcommand{\doctitle}{Apuntes}
% Subtítulo.
\newcommand{\docsubtitle}{}
% Fecha.
\newcommand{\docdate}{\date}
% Asignatura.
\newcommand{\subject}{Inferencia Estadística}
% Autor.
\newcommand{\docauthor}{Andrés Herrera Poyatos \\ Nuria Rodríguez Barroso \\ Juan Luis Suárez Díaz}
\newcommand{\docaddress}{Universidad de Granada}
\newcommand{\docemail}{}%andreshp9@gmail.com}


%-----------------------------------------------------------------------------------------------------
%	RESUMEN
%-----------------------------------------------------------------------------------------------------

% Resumen del documento. Va en la portada.
% Puedes también dejarlo vacío, en cuyo caso no aparece en la portada.
\newcommand{\docabstract}{}
%\newcommand{\docabstract}{En este texto puedes incluir un resumen del documento. Este informa al lector sobre el contenido del texto, indicando el objetivo del mismo y qué se puede aprender de él.}

\begin{document}

\hypersetup{pageanchor=false}
\maketitle
\hypersetup{pageanchor=true}

%-----------------------------------------------------------------------------------------------------
%	ÍNDICE
%-----------------------------------------------------------------------------------------------------

% Profundidad del Índice:
\setcounter{tocdepth}{2}

\newpage
\tableofcontents
\vspace*{\fill}
\doclicenseThis
\newpage

%-----------------------------------------------------------------------------------------------------
%	SECCIÓN 1
%-----------------------------------------------------------------------------------------------------

\importsection{Intro.tex}
\pagebreak

\section{Familias de distribuciones}

\subsection{Distribuciones discretas}

En esta sección se desarrollan varias de las distribuciones discretas más importantes de la estadística.

    \subsubsection{Distribución uniforme}
La distribución uniforme es una distribución de probabilidad que asume un número finito de valores con la misma probabilidad. Es fácil comprobar que la función masa de probabilidad es $f(x|n) = \frac{1}{n}$. Claramente $\sum^n_{i=1} \frac{1}{n} = 1$.

La función generatriz de momentos es fácil calcularla y viene definida por $\varphi_X(t) = \frac{e^t (1 - e^tN)}{N(1-e^t)}$. De ella podemos obtener su media y varianza las cuales quedan de la siguiente forma:

\begin{center}
	$E[X] = \frac{N+1}{2}$
	\\$Var(X) =  E[X^2] - (E[X])^2 =  \frac{(N+1)(N-1)}{12}$
\end{center}

\subsubsection{Distribución de Poisson}
Esta distribución expresa, a partir de una frecuencia de ocurrencia media, la probabilidad de que ocurra un determinado número de eventos durante cierto período de tiempo. La función de masa o probabilidad de la distribución de Poisson es $f(x| \lambda) = \frac{e^{-\lambda}{\lambda}^x}{x!}$. \\Claramente $\sum^n_{i=1}  \frac{e^{-\lambda}{\lambda}^x}{x!} = e^{-\lambda} \sum^n_{i=1}  \frac{{\lambda}^x}{x!}  = 1$.


La función generatriz de momentos de dicha distribución se calcula de la siguiente manera $\varphi_X(t) = \sum^n_{i=0} \frac{e^{tx}e^{-\lambda}{\lambda}^x}{x!} = e^{-\lambda} \sum^n_{i=0}   \frac{(e^{t}  \lambda)^x}{x!} =  e^{-\lambda}  e ^{e^{t} \lambda} = e ^{\lambda (e^t -1 )}$.

A partir de la función generatriz de momentos podemos fácilmente deducir la media y la varianza:
\begin{center}
	$E[X] = \lambda $
	\\$Var(X) =  E[X^2] - (E[X])^2 =  \lambda$
\end{center}

\subsubsection{Distribución binomial}

Considérese un experimento de Bernoulli con probabilidad $\theta \in [0,1]$. Repetimos el experimento $n$ veces y nos preguntamos cuál es la probabilidad de que se hayan conseguido $x$ aciertos, donde $x = 0, 1, \ldots, n$. Es fácil ver que esta probabilidad viene dada por $\binom{n}{x} \theta^x (1-\theta)^{n-x}$. Esta cuestión, que es habitual en la estadística, origina la distribución binomial.

\begin{definition}
    Una variable aleatoria sigue una distribución binomial con parámetros $n \in \mathbb{N}$ y $\theta \in [0,1]$  si su función masa de probabilidad viene dada por $f(x|n,\theta) = \binom{n}{x} \theta^x (1-\theta)^{n-x}$. En tal caso se denota $X \sim B(x|n,\theta)$.
\end{definition}

\subsubsection{Distribución multinomial}

\begin{definition}
	La distribución multinomial se puede ver como una generalización de la distribución binomial para variables politómicas (variables discretas con más de dos categorías).
	\\Supongamos que se realizan $n$ ensayos independientes que dan lugar a $k$ resultados distintos $X = (X_1, ... X_k)$  con probabilidades $\theta_1, ... , \theta_k$ respectivamente, donde $\theta_1+... + \theta_k = 1$. Entonces, la función de probabilidad para dicha variable multinomial sería:

	\[
	f(X|\theta_1, ... , \theta_k) = \frac{n!}{X_1! X_2! ... X_k!} \theta^{X_1} \theta^{X_2} ... \theta^{X_k}
	\]

\end{definition}


\subsection{Distribuciones continuas}

En esta sección se desarrollan varias de las distribuciones continuas más importantes de la estadística.

\subsubsection{Distribución uniforme}

La distribución uniforme asigna una credibilidad uniforme a todos los puntos de un intervalo $[a,b]$. Esto es, su función de densidad viene dada por
\[f(x|a,b) = \begin{cases}\frac{1}{b-a} \text{ si } x \in [a,b], \\ 0 \text{ en otro caso.}\end{cases}\]
Claramente tenemos que $\int_{-\infty}^{\infty} f(x |a,b) dx = 1$. Además, podemos calcular fácilmente sus momentos como sigue (y, por tanto, también su varianza)
\[E[X^j] = \int_a^b \frac{x^j}{b-a} dx = \frac{b^{j+1} - a^{j+1}}{(b-a) (j+1)},\]
\[Var(X) = E[X^2] - E[X]^2 = \frac{a^2 + ab + b^2}{3} - \frac{(a+b^2)}{4} = \frac{(b-a)^2}{12}.\]

\subsubsection{Distribución normal}

La distribución normal, también llamada distribución gaussiana, es la distribución más importante de la estadística. Esto se debe a sus numerosas aplicaciones en análisis de poblaciones y al teorema central del límite.

\begin{definition}
    Sean $\mu \in \mathbb{R}$ y $\sigma^2 > 0$. Definimos la distribución $N(x | \mu, \sigma^2)$ como la distribución que tiene función de densidad
    \[f(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}e^{-(x-\mu)^2 / (2\sigma^2)}, x \in \mathbb{R}.\]
\end{definition}

La distribución normal está bien definida como consecuencia del siguiente lema.
\begin{lem}
    Sean $\mu \in \mathbb{R}$ y $\sigma > 0$. Tenemos que $\int_{-\infty}^\infty e^{-(x-\mu)^2 / (2\sigma^2)}dx = \sqrt{2\pi}\sigma.$
\end{lem}
\begin{proof}
    En primer lugar, vamos a calcular la integral para $\mu = 0$ y $\sigma = 1$. La demostración consiste en reducir el problema en calcular una integral en dos variables. Para ello, elevamos al cuadrado y obtenemos
    \[\left(\int_{-\infty}^\infty e^{-x^2 / 2}dx\right)^2 = \left(\int_{-\infty}^\infty e^{-t^2 / 2}dt\right) \left(\int_{-\infty}^\infty e^{-s^2 / 2}ds \right) = \int_{-\infty}^\infty \int_{-\infty}^\infty e^{-(t^2+s^2) / 2} dt ds. \]
    Resolvemos esta última integral mediante un cambio a polares
    \[\int_{-\infty}^\infty \int_{-\infty}^\infty e^{-(t^2+s^2) / 2} dt ds = \int_{-\pi}^\pi \left(\int_{0}^\infty \rho e^{-\rho^2 / 2} d\rho \right) d\theta = 2\pi \int_{0}^\infty \rho e^{-\rho^2 / 2} d\rho = 2\pi.\]
    Por último, utilizamos el cambio de variable $y = (x - \mu) / \sigma$ para obtener
    \[\int_{-\infty}^\infty e^{-(x-\mu)^2 / (2\sigma^2)}dx = \int_{-\infty}^\infty \sigma e^{-y^2 / 2}dy = \sqrt{2\pi}\sigma. \qedhere\]
\end{proof}

Nótese que si $X \sim N(x | \mu, \sigma^2)$, entonces $Y = (X - \mu)/\sigma$ sigue una distribución $N(x|0,1)$.

\begin{prop} \label{prop:normal:cf}
    La función característica de la distribución $N(x|\mu, \sigma^2)$ viene dada por $\varphi_X(t) = e^{it\mu - t^2 \sigma^2 / 2}$.
\end{prop}
\begin{proof}
    En primer lugar, tenemos que
    \[\varphi_X(t) = E[e^{itX}] = \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^{\infty} e^{itx-(x-\mu)^2 / (2\sigma^2)} dx = \frac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^{\infty} e^{-((x-\mu)^2 - 2itx\sigma^2) / (2\sigma^2)} dx.\]
    Completamos cuadrados como sigue
    \[(x-\mu)^2 - 2 itx\sigma^2 = (x - (it \sigma^2 + \mu))^2 + t^2 \sigma^4 - 2it\sigma^2 \mu.\]
    Esto sugiere utilizar el cambio de variable $g(y) = y + it \sigma^2$. Obtenemos
    \begin{align*}
        \sqrt{2\pi}\sigma \varphi_X(t) = \int_{-\infty}^{\infty} e^{-((x-\mu)^2 - 2itx\sigma^2) / (2\sigma^2)} = e^{it\mu - t^2 \sigma^2 / 2} \int_{-\infty}^{\infty} e^{-((x-(it \sigma^2 + \mu))^2 ) / (2\sigma^2)} dx \\
        = e^{it\mu - t^2 \sigma^2 / 2} \int_{-\infty}^{\infty} e^{-(y - \mu)^2 ) / (2\sigma^2)} dy = \sqrt{2\pi}\sigma e^{it\mu - t^2 \sigma^2 / 2},
    \end{align*}
    como se quería.
    Nótese que a pesar de ser una integral de contorno compleja el cambio de variable es válido. En efecto, el cambio de variable es afín y la función a integrar es entera. Por tanto, utilizando el camino cerrado $g([0, \infty]) + [\infty, 0]$ se puede probar que el cambio es válido.
\end{proof}

Análogamente se puede probar el siguiente resultado.

\begin{prop} \label{prop:normal:gm}
    La función generatriz de momentos de la distribución $N(x|\mu, \sigma^2)$ viene dada por $\varphi_X(t) = e^{t\mu - t^2 \sigma^2 / 2}$.
\end{prop}

\begin{cor} \label{cor:normal:rec}
    Los momentos de la distribución $N(x|\mu,\sigma^2)$ verifican la ecuación recurrente
    \[E[X^k] = -(k-1)\sigma^2 E[X^{k-2}] + (\mu - t \sigma^2) E[X^{k-1}], \ \  k \ge 2.\]
\end{cor}
\begin{proof}
    Sabemos que $E[X^k] = \varphi_X^{(k)}(t)$. Tenemos $\varphi_X^{(1)}(t) = (\mu - t \sigma^2) \varphi_X(t)$. Consecuentemente,
    \[\varphi_X^{(2)}(t) = -\sigma^2 \varphi_X(t) + (\mu - t \sigma^2) \varphi_X^{(1)}(t).\]
    Por inducción se extiende el resultado fácilmente para $k \ge 2$.
\end{proof}

\begin{cor}
    Si $X \sim N(x|\mu,\sigma^2)$, entonces $E[X] = \mu$ y $E[X^2] = \sigma^2 + \mu^2$. Consecuentemente, $Var(X) = \sigma^2$. Como consecuencia de este resultado al parámetro $\mu$ se le llama media y al parámetro $\sigma^2$ varianza.
\end{cor}

Podemos utilizar los dos corolarios anteriores para calcular los momentos de la distribución normal resolviendo una ecuación recurrente de segundo orden. Evidentemente, la fórmula obtenida será bastante larga. Sin embargo, esta ecuación se simplifica en el caso de los momentos centrados, como pone de manifiesto el siguiente resultado, que se puede demostrar fácilmente por inducción a partir del Corolario \ref{cor:normal:rec}.

\begin{cor}
    Si $X \sim N(x|0,\sigma^2)$, entonces
    \[E[X^k] = \begin{cases} 0 & \text{ si } k \text{ es impar;} \\ (k-1)!! \sigma^{k} & \text{ si } k \text{ es par;} \end{cases}\]
    donde $n!!$ denota al doble factorial, definido como el producto de los números desde $1$ hasta $n$ con la misma paridad que $n$.
\end{cor}

\begin{comment}

La Figura \ref{fig:normal} muestra la función de densidad de una distribución normal. Podemos ver que la densidad se concentra en torno a la media. De hecho, $P(|X - \mu| \ge 2\sigma) \approx 0.046$. Es más, $P(|X - \mu| \ge 3\sigma) \approx 0.03$.

\begin{figure}[H]
\centering
\begin{tikzpicture}[
     declare function={gauss(\x,\mu,\sigma)=1/(\sigma*sqrt(2*pi))*exp(-((\x-\mu)^2)/(2*\sigma^2));}
    ]
\begin{axis}[
    no markers,
    domain=0:6,
    samples=100,
    ymin=0,
    axis lines*=left,
    xlabel=$x$,
    every axis y label/.style={at=(current axis.above origin),anchor=south},
    every axis x label/.style={at=(current axis.right of origin),anchor=west},
    height=5cm,
    width=15cm,
    xtick=\empty,
    ytick=\empty,
    enlargelimits=false,
    clip=false,
    axis on top,
    grid = major,
    hide y axis
]

\addplot[very thick,cyan!50!black] {gauss(x, 3, 1)};

        \pgfmathsetmacro\valueA{gauss(1,3,1)}
        \pgfmathsetmacro\valueB{gauss(2,3,1)}
        \draw [gray] (axis cs:1,0) -- (axis cs:1,\valueA)
            (axis cs:5,0) -- (axis cs:5,\valueA);
            \draw [gray] (axis cs:2,0) -- (axis cs:2,\valueB)
            (axis cs:4,0) -- (axis cs:4,\valueB);
            \draw [yshift=1.4cm, latex-latex](axis cs:2, 0) -- node [fill=white] {$0.683$} (axis  cs:4, 0);
            \draw [yshift=0.3cm, latex-latex](axis cs:1, 0) -- node [fill=white] {$0.954$} (axis cs:5, 0);

            \node[below] at (axis cs:1, 0)  {$\mu - 2\sigma$};
            \node[below] at (axis cs:2, 0)  {$\mu - \sigma$};
            \node[below] at (axis cs:3, 0)  {$\mu$};
            \node[below] at (axis cs:4, 0)  {$\mu + \sigma$};
            \node[below] at (axis cs:5, 0)  {$\mu + 2\sigma$};
\end{axis}

\end{tikzpicture}
\caption{Función de densidad de una distribución normal.}
\label{fig:normal}
\end{figure}

\begin{prop}
    Sean $X_1$ e $Y_2$ dos variables aleatorias independientes que siguen una distribución $N(x|\mu_1,\sigma_1^2)$ y $N(x|\mu_2,\sigma_2^2)$ respectivamente. Entonces $X+Y$ sigue una distribución $N(x|\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$.
\end{prop}
\begin{proof}
    Basta darse cuenta de que $\varphi_{X+Y}(t) = \varphi_{X}(t)\varphi_{Y}(t) = e^{t(\mu_1 + \mu_2) - t^2 (\sigma_1^2 + \sigma_2^2) / 2}$ es la función característica asociada a la distribución $N(x|\mu_1+\mu_2,\sigma_1^2+\sigma_2^2)$. Recordemos que la función característica determina de forma unívoca a la distribución.
\end{proof}

El recíproco del resultado anterior también es cierto.

\begin{thm}[Cramer]
    Sean $X$ e $Y$ dos variables aleatorias independientes. Si $X+Y$ es normal, entonces $X$ e $Y$ son normales.
\end{thm}

\subsubsection{Distribución gamma}

La famila de distribuciones gamma se encuentra definida sobre el intervalo $[0, \infty)$. En su definición entra en juego la famosa función gamma, de ahí su nombre.

\begin{definition}
    Se define la función gamma como la aplicación $\Gamma: (0, \infty) \to (0, \infty)$ dada por
    \[\Gamma(\alpha) = \int_0^\infty t^{\alpha-1}e^{-t}dt.\]
\end{definition}
\begin{prop}
    La función gamma está bien definida.
\end{prop}
\begin{proof}
    Sea $\alpha > 0$. Tenemos que probar que $\int_0^\infty t^{\alpha-1}e^{-t}dt < \infty$. Tomando $b > 0$, escribimos
    \[\int_0^\infty t^{\alpha-1}e^{-t}dt = \int_0^b t^{\alpha-1}e^{-t}dt + \int_b^\infty t^{\alpha-1}e^{-t}dt.\]
    Sabemos que la función $t^{\alpha - 1}$ tiene a $t^{\alpha} / \alpha$ como primitiva y, por tanto, es integrable en $[0,b]$. Puesto que $t^{\alpha-1}e^{-t} \le t^{\alpha-1}$, obtenemos que $t^{\alpha-1}e^{-t}$ es integrable en $[0,b]$. Por otro lado tenemos que
    \[\lim_{t \to \infty} \frac{t^{\alpha-1}e^{-t}}{e^{-t / 2}} = 0.\]
    Consecuentemente, para cierto $b > 0$ se verifica $t^{\alpha-1}e^{-t} \le e^{-t / 2}$ para todo $t \ge b$. Puesto que $e^{-t / 2}$ es integrable en $[b, \infty)$, deducimos que $t^{\alpha-1}e^{-t}$ también lo es, lo que termina la demostración.
\end{proof}

\begin{prop}[Propiedades de la función gamma]
    Sea $\alpha > 0$. Se verifica:
    \begin{enumerate}
        \item $\Gamma(1) = 1$;
	\item $\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$;
    \item $\Gamma(n+1) = n!$ para cualquier $n \in \mathbb{N}$;
    \item (Fórmula de reflexión de Euler) si $0 < \alpha < 1$, entonces $\Gamma(\alpha) \Gamma(1- \alpha) = \frac{\pi}{\sin(\alpha \pi)}$;
    \item $\Gamma(1/2) = \sqrt{\pi}$;
    \item $\Gamma(\alpha) = \beta^\alpha\int_0^\infty t^{\alpha-1}e^{-\beta t} dt$ para todo $\beta > 0$.
    \end{enumerate}
\end{prop}
\begin{proof}
    \
    \begin{enumerate}
        \item Es fácil ver que $\int_{0}^\infty e^{-t} dt = 1$.
        \item Integrando por partes obtemos
        \[\Gamma(\alpha+1) = \int_0^{\infty}{t^{\alpha}e^{-t}dt} = \bigg[-e^{-t}t^\alpha\bigg]_0^{\infty} +
            \int_0^{\infty}{xt^{\alpha-1}e^{-t}dt} = \alpha\int_0^{\infty}{t^{\alpha-1}e^{-t}dt} = \alpha\Gamma(\alpha).\]
        \item Es consecuencia directa de los apartados a) y b).
        \item Se obtiene utilizando definiciones alternativas de la función gamma tras extenderla a $\mathbb{C} \setminus \mathbb{Z}^-_0$. Para más información véase \cite{gamma}. No desarrollamos esta demostración pues solo la necesitamos para el siguiente apartado.
        \item Se obtiene al evaluar la fórmula de reflexión en $\alpha = 1/2$.
        \item Se obtiene realizando el cambio de variable $t = \beta s$. \qedhere
    \end{enumerate}
\end{proof}

\begin{definition}
    Sean $\alpha, \beta > 0$. Definimos la distribución $Gamma(x | \alpha, \beta)$ como la distribución que tiene función de densidad
    \[f(x | \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}, x > 0.\]
\end{definition}

El parámetro $\alpha$ se conoce como parámetro de forma ya que influencia la forma de la distribución, como muestra el siguiente resultado.

\begin{prop}
    La función de densidad de la distribución $Gamma(\alpha, \beta)$ verifica las siguientes propiedades:
    \begin{itemize}
        \item Si $0< \alpha <1$, entonces $f(x | \alpha, \beta)$ es decreciente y $f(x) \to \infty$ para $x \to 0$.
        \item Si $\alpha = 1$, entonces $f(x | \alpha, \beta)$ es decreciente con $f(0) = 1$.
        \item Si $\alpha > 1$, entonces $f(x | \alpha, \beta)$ crece en $[0, (\alpha-1) / \beta]$ y decrece en $[(\alpha-1) / \beta,\infty]$.
        \item Si $0 < \alpha \le 1$, entonces $f(x | \alpha, \beta)$ es convexa.
        \item Si $1 < \alpha \le 2$, entonces $f(x | \alpha, \beta)$ es cóncava en $[0,(\alpha-1 + \sqrt{\alpha - 1}) / \beta]$ y convexa en $[(\alpha-1 + \sqrt{\alpha - 1}) / \beta, \infty]$.
        \item Si $2 < \alpha$, entonces $f(x | \alpha, \beta)$ es cóncava en $[(\alpha-1 - \sqrt{\alpha - 1}) / \beta,(\alpha-1 + \sqrt{\alpha - 1}) / \beta]$ y convexa en $[0, (\alpha-1 - \sqrt{\alpha - 1}) / \beta]$ y $[(\alpha-1 + \sqrt{\alpha - 1}) / \beta, \infty]$.
    \end{itemize}
\end{prop}
\begin{proof}
Los resultados se obtienen mediante las herramientas habituales del cálculo. Basta estudiar la derivada primera y la derivada segunda
\begin{align*}
f^\prime(x) &= \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-2} e^{-\beta x}[(\alpha - 1) - \beta x]; \\
f^{\prime \prime}(x) &= \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-3} e^{-\beta x} \left[(\alpha - 1)(\alpha - 2) - 2 \beta (\alpha - 1) x + \beta^2 x^2\right]. \qedhere
\end{align*}
\end{proof}

La Figura \ref{fig:gamma:alpha} muestra la función de densidad de la distribución gamma para distintos valores de $\alpha$.  El parámetro $\beta$ se denomina parámetro de escala debido a su influencia en la escala de la función de densidad. La Figura \ref{fig:gamma:beta} muestra la función de densidad de la distribución gamma para distintos valores de $\beta$.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        declare function={gamma(\z)=
        2.506628274631*sqrt(1/\z)+ 0.20888568*(1/\z)^(1.5)+ 0.00870357*(1/\z)^(2.5)- (174.2106599*(1/\z)^(3.5))/25920- (715.6423511*(1/\z)^(4.5))/1244160)*exp((-ln(1/\z)-1)*\z;},
        declare function={gammapdf(\x,\k,\theta) = 1/(\theta^\k)*1/(gamma(\k))*\x^(\k-1)*exp(-\x/\theta);}
        ]
        \begin{axis}[
            ylabel={$f(x | \alpha, \beta)$},
            domain=0.000001:10, samples=100,
            axis lines=left,
            every axis y label/.style={at=(current axis.above origin),anchor=east},
            every axis x label/.style={at=(current axis.right of origin),anchor=north},
            height=6cm, width=12cm,
            enlargelimits=false,
            clip=false,
            axis on top,
            grid = major,
            legend pos=outer north east
        ]
            \addplot [very thick,cyan!20!black] {gammapdf(x,0.98,2)};
            \addlegendentry{$\alpha = 0.9, \beta = 2$}
            \addplot [very thick,cyan!35!black] {gammapdf(x,1,2)};
            \addlegendentry{$\alpha = 1, \beta = 2$}
            \addplot [very thick,cyan!60!black] {gammapdf(x,2,2)};
            \addlegendentry{$\alpha = 2, \beta = 2$}
            \addplot [very thick,cyan] {gammapdf(x,3,2)};
            \addlegendentry{$\alpha = 3, \beta = 2$}
        \end{axis}
    \end{tikzpicture}
    \caption{Densidad de la distribución gamma con distintos valores de $\alpha$.}
    \label{fig:gamma:alpha}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        declare function={gamma(\z)=
        2.506628274631*sqrt(1/\z)+ 0.20888568*(1/\z)^(1.5)+ 0.00870357*(1/\z)^(2.5)- (174.2106599*(1/\z)^(3.5))/25920- (715.6423511*(1/\z)^(4.5))/1244160)*exp((-ln(1/\z)-1)*\z;},
        declare function={gammapdf(\x,\k,\theta) = 1/(\theta^\k)*1/(gamma(\k))*\x^(\k-1)*exp(-\x/\theta);}
        ]
        \begin{axis}[
            ylabel={$f(x | \alpha, \beta)$},
            domain=0:10, samples=100,
            axis lines=left,
            every axis y label/.style={at=(current axis.above origin),anchor=east},
            every axis x label/.style={at=(current axis.right of origin),anchor=north},
            height=6cm, width=12cm,
            enlargelimits=false, clip=false, axis on top,
            grid = major,
            legend pos=outer north east
        ]
            \addplot [very thick,magenta!10!black] {gammapdf(x,2,0.5)};
            \addlegendentry{$\alpha = 2, \beta = 0.5$}
            \addplot [very thick,magenta!40!black] {gammapdf(x,2,1)};
            \addlegendentry{$\alpha = 2, \beta = 1$}
            \addplot [very thick,magenta!75!black] {gammapdf(x,2,2)};
            \addlegendentry{$\alpha = 2, \beta = 2$}
            \addplot [very thick,magenta] {gammapdf(x,2,3)};
            \addlegendentry{$\alpha = 2, \beta = 3$}
        \end{axis}
    \end{tikzpicture}
    \caption{Densidad de la distribución gamma con distintos valores de $\beta$.}
    \label{fig:gamma:beta}
\end{figure}

\begin{prop} \label{prop:gamma:cf}
    La función característica de la distribución $Gamma(x|\alpha, \beta)$ viene dada por $\varphi_X(t) = \left(\frac{\beta}{\beta -it}\right)^\alpha$.
\end{prop}
\begin{proof}
    Basta utilizar el cambio de variable $g(y) = y / (\beta - it)$ como sigue
    \[E[e^{itX}] = \frac{\beta^\alpha}{\Gamma(\alpha)} \int_{0}^{\infty} x^{\alpha-1}e^{-(\beta - it) x} dx = \frac{\beta^\alpha}{\Gamma(\alpha) (\beta -it)^\alpha} \int_{0}^{\infty} y^{\alpha-1}e^{-y} dy = \left(\frac{\beta}{\beta -it}\right)^\alpha.\]
    Nótese que a pesar de ser una integral de contorno compleja el cambio de variable afín es válido como se comentó en la Proposición \ref{prop:normal:cf}.
\end{proof}

\begin{cor}
    El momento $k$-ésimo de la distribución $Gamma(x|\alpha,\beta)$ es $\alpha (\alpha+1) \ldots (\alpha + k -1) / \beta^k$.
\end{cor}
\begin{proof}
    Tenemos que $i^k E[X^k]= \varphi_X^{(k)}(t) = i^k \alpha (\alpha+1) \ldots (\alpha + k -1) / \beta^k$.
\end{proof}

\begin{prop}
    La función generatriz de momentos de la distribución $Gamma(x|\alpha, \beta)$ viene dada por $\varphi_X(t) = \left(\frac{\beta}{\beta - t}\right)^\alpha$.
\end{prop}
\begin{proof}
    La demostración es análoga a la dada en la Proposición \ref{prop:gamma:cf}.
\end{proof}

\begin{cor}
    La distribución $Gamma(x|\alpha,\beta)$ tiene media $\alpha / \beta$ y varianza $\alpha / \beta^2$.
\end{cor}

\begin{prop}
    Sea $n \ge 1$. Consideremos $X_1, \ldots, X_n$ variables aleatorias independientes tales que $X_j$ sigue una distribución $Gamma(x|\alpha_i, \beta)$. Entonces, $\sum_{i=1}^n X_j$ sigue una distribución $Gamma(x|\sum_{i=1}^n \alpha_i, \beta)$.
\end{prop}
\begin{proof}
    En primer lugar, calculamos la función característica de $\sum_{i=1}^n X_j$ como sigue
    \[E[e^{i\sum X_j}] = E[\prod e^{iX_j}] = \prod E[e^{iX_j}] = \left(\frac{\beta}{\beta - it}\right)^{\sum \alpha_j},\]
    donde se ha utilizado que la esperanza del producto de dos variables aleatorias independientes es el producto de las esperanzas. Por último, nótese que la función característica de la variable $\sum X_j$ es la función característica de $Gamma(x|\sum_{i=1}^n \alpha_i, \beta)$. El hecho de que la función característica de una distribución la determina de forma unívoca finaliza la prueba.
\end{proof}

\begin{prop} \label{prop:normal-square}
    Sea $X \sim N(x|0,\sigma^2)$. La variable aleatoria $Y = X^2$ sigue una distribución \\ $Gamma(y,1/2,1/(2\sigma^2))$. En particular, para $\sigma = 1$, $Y = X^2$ sigue una distribución $\chi^2_1$.
\end{prop}
\begin{proof}
    Sean $F$ y $G$ las funciones de distribución de las variables $X$ e $Y$ respectivamente. Tenemos que $G(y) = P(X^2 \le y) = P(- \sqrt{y} \le X \le \sqrt{y}) = F(\sqrt{y}) - F(-\sqrt{y})$. Derivando, obtenemos
    \[G'(y) = \frac{F'(\sqrt{y}) + F'(-\sqrt{y})}{2\sqrt{y}} = \frac{1}{\sqrt{y}} \frac{1}{\sqrt{2\pi}\sigma} e^{-y/(2\sigma^2)} = \frac{(1/(2\sigma^2))^{1/2}}{\Gamma(1/2)} y^{-1/2} e^{-y/(2\sigma^2)}.\]
    Por último, basta darse cuenta de que $G'(y)$ es la función de densidad de $Gamma(y,1/2,1/(2\sigma^2))$.
\end{proof}

\subsubsection{Distribución beta}

La famila de distribuciones beta se encuentra definida sobre el intervalo $(0, 1)$. En su definición entra en juego la denominada función beta, de ahí su nombre.

\begin{definition}
    Se define la función beta como la aplicación $\beta : (0, \infty) \times (0, \infty) \to (0, \infty)$ dada por
    \[\beta(x, y) = \int_0^1 t^{x-1}(1-t)^{y-1}\,dt.\]
\end{definition}
\begin{prop} \label{prop:beta-gamma}
    Para cada $x, y > 0$ se tiene que $\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)} = \beta(x,y)$. Como consecuencia, la función beta está bien definida.
\end{prop}
\begin{proof}
    En primer lugar escribimos $\Gamma(x)\Gamma(y)$ como una integral doble
    \begin{equation*}
        \Gamma(x)\Gamma(y) =\int_{0}^{\infty }\ e^{-u}u^{x-1}\,d u\int_{0}^{\infty }\ e^{-v}v^{y-1}\,d v
        =\int_{0}^{\infty }\int_{0}^{\infty }\ e^{-u-v}u^{x-1}v^{y-1}\,d u\,d v.
    \end{equation*}
    La expresión anterior nos sugiere utilizar el cambio de variable $(u, v) = J(t,s) = (st, (1-t)s)$. Nótese que $|J(t,s)| = s$. Aplicamos el cambio a continuación
    \begin{gather*}
        \Gamma(x)\Gamma(y) = \int_{0}^{\infty} \left( \int_{0}^{1}e^{-s}(st)^{x-1}(s(1-t))^{y-1}|J(t,s)|\, d t \right) ds \\
        = \int_{0}^{\infty }e^{-s}s^{x+y-2}s \left(\int_{0}^{1}t^{x-1}(1-t)^{y-1}\,dt \right) d s =\Gamma(x+y)\beta(x,y).  \qedhere
    \end{gather*}
\end{proof}

En la práctica siempre se utiliza la función gamma para evaluar la función beta. Ya podemos definir la distribución beta.

\begin{definition}
    Sean $p, q > 0$. Definimos la distribución $beta(x | p, q)$ como la distribución que tiene función de densidad
    \[f(x | p, q) = \frac{1}{\beta(p,q)}x^{p-1}(1-x)^{q-1}, 0 < x < 1.\]
\end{definition}

Claramente, la función de densidad integra $1$. Esta distribución asigna probabilidad $1$ al intervalo $(0,1)$. Por ello, es útil en modelos de proporciones. Las Figuras \ref{fig:beta:p} y \ref{fig:beta:q} muestran la distribución beta cambiando los valores $p$ y $q$ respectivamente. Podemos observar que las funciones de densidad de $beta(x|p,q)$ y $beta(x|q,p)$ son simétricas respecto del punto $1/2$. Esto se puede demostrar fácilmente a partir de la definición. La Figura \ref{fig:beta:pq} muestra la distribución beta con iguales valores de $p$ y $q$. Vemos que las densidades son simétricas en el eje $x = 1/2$, hecho que también puede demostrarse fácilmente a partir de la definición.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        declare function={gamma(\z)=
        2.506628274631*sqrt(1/\z)+ 0.20888568*(1/\z)^(1.5)+ 0.00870357*(1/\z)^(2.5)- (174.2106599*(1/\z)^(3.5))/25920- (715.6423511*(1/\z)^(4.5))/1244160)*exp((-ln(1/\z)-1)*\z;},
        declare function={betapdf(\x,\p,\q) = gamma(\p+\q)/(gamma(\p)*gamma(\q)) * \x^(\p-1)*(1-x)^(\q-1);}
        ]
        \begin{axis}[
            ylabel={$f(x | p, q)$},
            domain=0.05:1, samples=100,
            axis lines=left,
            every axis y label/.style={at=(current axis.above origin),anchor=east},
            every axis x label/.style={at=(current axis.right of origin),anchor=north},
            height=6cm, width=12cm,
            enlargelimits=false, clip=false, axis on top,
            grid = major,
            legend pos=outer north east
        ]
            \addplot [very thick,cyan!10!black] {betapdf(x,0.5,2)};
            \addlegendentry{$p = 0.5, q = 2$}
            \addplot [very thick,cyan!40!black] {betapdf(x,1,2)};
            \addlegendentry{$p = 1, q = 2$}
            \addplot [very thick,cyan!75!black] {betapdf(x,2,2)};
            \addlegendentry{$p = 2, q = 2$}
            \addplot [very thick,cyan] {betapdf(x,3,2)};
            \addlegendentry{$p = 3, q = 2$}
        \end{axis}
    \end{tikzpicture}
    \caption{Densidad de la distribución beta con distintos valores de $p$.}
    \label{fig:beta:p}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        declare function={gamma(\z)=
        2.506628274631*sqrt(1/\z)+ 0.20888568*(1/\z)^(1.5)+ 0.00870357*(1/\z)^(2.5)- (174.2106599*(1/\z)^(3.5))/25920- (715.6423511*(1/\z)^(4.5))/1244160)*exp((-ln(1/\z)-1)*\z;},
        declare function={betapdf(\x,\p,\q) = gamma(\p+\q)/(gamma(\p)*gamma(\q)) * \x^(\p-1)*(1-x)^(\q-1);}
        ]
        \begin{axis}[
            ylabel={$f(x | p, q)$},
            domain=0:0.95, samples=100,
            axis lines=left,
            every axis y label/.style={at=(current axis.above origin),anchor=east},
            every axis x label/.style={at=(current axis.right of origin),anchor=north},
            height=6cm, width=12cm,
            enlargelimits=false, clip=false, axis on top,
            grid = major,
            legend pos=outer north east
        ]
            \addplot [very thick,magenta!10!black] {betapdf(x,2,0.5)};
            \addlegendentry{$p = 2, q = 0.5$}
            \addplot [very thick,magenta!40!black] {betapdf(x,2,1)};
            \addlegendentry{$p = 2, q = 1$}
            \addplot [very thick,magenta!75!black] {betapdf(x,2,2)};
            \addlegendentry{$p = 2, q = 2$}
            \addplot [very thick,magenta] {betapdf(x,2,3)};
            \addlegendentry{$p = 2, q = 3$}
        \end{axis}
    \end{tikzpicture}
    \caption{Densidad de la distribución beta con distintos valores de $q$.}
    \label{fig:beta:q}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        declare function={gamma(\z)=
        2.506628274631*sqrt(1/\z)+ 0.20888568*(1/\z)^(1.5)+ 0.00870357*(1/\z)^(2.5)- (174.2106599*(1/\z)^(3.5))/25920- (715.6423511*(1/\z)^(4.5))/1244160)*exp((-ln(1/\z)-1)*\z;},
        declare function={betapdf(\x,\p,\q) = gamma(\p+\q)/(gamma(\p)*gamma(\q)) * \x^(\p-1)*(1-x)^(\q-1);}
        ]
        \begin{axis}[
            ylabel={$f(x | p, q)$},
            domain=0.05:0.95, samples=100,
            axis lines=left,
            every axis y label/.style={at=(current axis.above origin),anchor=east},
            every axis x label/.style={at=(current axis.right of origin),anchor=north},
            height=6cm, width=12cm,
            enlargelimits=false, clip=false, axis on top,
            grid = major,
            legend pos=outer north east
        ]
            \addplot [very thick,green!10!black] {betapdf(x,0.5,0.5)};
            \addlegendentry{$p = 0.5, q = 0.5$}
            \addplot [very thick,green!40!black] {betapdf(x,1,1)};
            \addlegendentry{$p = 1, q = 1$}
            \addplot [very thick,green!75!black] {betapdf(x,2,2)};
            \addlegendentry{$p = 2, q = 2$}
            \addplot [very thick,green] {betapdf(x,3,3)};
            \addlegendentry{$p = 3, q = 3$}
        \end{axis}
    \end{tikzpicture}
    \caption{Densidad de la distribución beta con $p = q$.}
    \label{fig:beta:pq}
\end{figure}

\begin{prop}
    La función característica de la distribución $beta(x|p,q)$ viene dada por
    \[\varphi_X(t) = 1 + \sum_{k = 1}^\infty \frac{(it)^k}{k!} \frac{\beta(p+k,q)}{\beta(p,q)}.\]
\end{prop}

\begin{proof}
Desarrollamos la función característica utilizando el desarrollo de la exponencial
\begin{gather*}
E[e^{itX}] = \frac{1}{\beta(p,q)}\int_0^1 e^{itx} x^{p-1}(1-x)^{q-1} dx = \frac{1}{\beta(p,q)}\int_0^1 \sum_{k = 0}^\infty \frac{(itx)^{k}}{k!} x^{p-1}(1-x)^{q-1} dx \\
= \frac{1}{\beta(p,q)}\sum_{k = 0}^\infty \frac{(it)^k}{k!} \int_0^1x^{p+k-1}(1-x)^{q-1} dx = \sum_{k = 0}^\infty \frac{(it)^k}{k!} \frac{\beta(p+k,q)}{\beta(p,q)} = 1 + \sum_{k = 1}^\infty \frac{(it)^k}{k!} \frac{\beta(p+k,q)}{\beta(p,q)}. \qedhere
\end{gather*}
\end{proof}

\begin{cor} \label{cor:beta:moments}
    Sea $X \sim beta(x|p,q)$. Entonces, para cada $k \ge 1$ se tiene que
    \[E[X^k] = \frac{\beta(p+k,q)}{\beta(p,q)}.\]
\end{cor}

\begin{cor} \label{cor:beta:esp}
    Sea $X \sim beta(x|p,q)$. Entonces, $E[X] = \frac{p}{p+q}$ y $Var(X) = \frac{pq}{(p+q)^2(p+q+1)}$.
\end{cor}
\begin{proof}
    Por el Corolario \ref{cor:beta:moments} y la Proposición \ref{prop:beta-gamma} tenemos que
    \[E[X] = \frac{\beta(p+1,q)}{\beta{p,q}} = \frac{\Gamma(p+1)\Gamma(q)}{\Gamma(p+q)} \frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)} = \frac{\Gamma(p+1)}{\Gamma(p)} \frac{\Gamma(p+q)}{\Gamma(p+q+1)} = \frac{p}{p+q}\]
    y
    \[E[X^2] = \frac{\beta(p+2,q)}{\beta{p,q}} = \frac{\Gamma(p+2)\Gamma(q)}{\Gamma(p+q+2)} \frac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)} = \frac{\Gamma(p+2)}{\Gamma(p)} \frac{\Gamma(p+q)}{\Gamma(p+q+2)} = \frac{p(p+1)}{(p+q)(p+q+1)}.\]
    Por último, es directo calcular $Var(X) = E[X^2] - E[X]^2$.
\end{proof}

\subsubsection{Distribución de Cauchy}

\begin{definition}
    Sea $\mu \in \mathbb{R}$ y $\sigma > 0$. Definimos la distribución $Cauchy(x | \mu, \sigma)$ como la distribución que tiene función de densidad
    \[f(x | \mu, \sigma) = \frac{1}{\sigma \pi} \frac{1}{1+\left(\frac{x-\mu}{\sigma}\right)^2} = \frac{\sigma}{\pi(\sigma^2 + (x-\mu)^2)}, \ x \in \mathbb{R}.\]
\end{definition}

La distribución está bien definida. En efecto, utilizando el cambio de variable $x = \sigma y + \mu$ obtenemos
\[\int_{-\infty}^\infty \frac{1}{1+\left(\frac{x-\mu}{\sigma}\right)^2} dx = \int_{-\infty}^\infty \frac{\sigma}{1+y^2} dy = \sigma \pi.\]

\begin{prop}
    La función característica de la distribución $Cauchy(x|\mu,\sigma)$ viene dada por
    \[\varphi_X(t) = e^{i\mu t - \sigma |t|}.\]
\end{prop}
\begin{proof}
    En primer lugar, demostramos el resultado para $Cauchy(x|0,1)$. Tenemos que
    \[\varphi_X(t)  = \frac{1}{\pi}\int_{-\infty}^\infty \frac{e^{itz}}{1+z^2} dz = e^{-|t|},\]
    donde la última igualdad se explica en \cite{cauchy}. Ahora, si $X \sim Cauchy(x|\mu,\sigma)$, entonces $Y = (X - \mu) / \sigma \sim Cauchy(x|0,1)$ y, por tanto, obtenmos
    \[\varphi_X(t) = E[e^{itX}] = E[e^{it(\sigma Y+\mu)}] = e^{it\mu} \varphi_Y(\sigma t) = e^{i\mu t - \sigma |t|}. \qedhere\]
\end{proof}

Nótese que la función característica de la distribución de Cauchy no es diferenciable en $0$. Consecuentemente, esta distribución no tiene momentos de orden mayor o igual que 1. El recíproco no sería cierto, esto es, existen distribuciones que no tienen esperanza y su función característica es diferenciable en $0$ \cite{char}.

\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias independientes con distribuciones $Cauchy(x|\mu_1, \sigma_1)$ y $Cauchy(x|\mu_2, \sigma_2)$ respectivamente. Entonces, $X+Y \sim Cauchy(x|\mu_1+\mu_2,\sigma_1+\sigma_2)$.
\end{prop}
\begin{proof}
    Nótese que $\varphi_{X+Y}(t) = \varphi_{X}(t)\varphi_{Y}(t) = e^{it(\mu_1+\mu_2) - |t| (\sigma_1+\sigma_2)}$. La prueba finaliza al darse cuenta de que ésta es la función característica de $Cauchy(x|\mu_1+\mu_2,\sigma_1+\sigma_2)$.
\end{proof}

\begin{figure}[H]
    \begin{tikzpicture}[
            declare function={gauss(\x,\mu,\sigma)=1/(\sigma*sqrt(2*pi))*exp(-((\x-\mu)^2)/(2*\sigma^2));},
            declare function={cauchy(\x,\mu,\sigma) = 1/((\sigma*pi)*(1+((\x-\mu)/\sigma)^2));}
        ]
        \begin{axis}[
            domain=-5:5, samples=100,
            axis lines=left,
            every axis y label/.style={at=(current axis.above origin),anchor=east},
            every axis x label/.style={at=(current axis.right of origin),anchor=north},
            height=6cm, width=12cm,
            enlargelimits=false, clip=false, axis on top,
            grid = major,
            legend pos=outer north east
        ]
            \addplot [very thick,cyan!70!black] {cauchy(x,0,1)};
            \addlegendentry{Cauchy, $\mu = 0, \sigma = 1$}
            \addplot [very thick,cyan!40!black] {cauchy(x,0,2)};
            \addlegendentry{Cauchy, $\mu = 0, \sigma = 2$}
            \addplot [very thick,magenta!75!black] {gauss(x,0,1)};
            \addlegendentry{Normal, $\mu = 0, \sigma = 1$}
        \end{axis}
    \end{tikzpicture}
    \caption{Densidad de la distribución de Cauchy comparada con la distribución normal.}
    \label{fig:cauchy}
\end{figure}


\subsubsection{Distribución de Laplace}

\begin{definition}
    Sea $\mu \in \mathbb{R}$ y $\sigma > 0$. Definimos la distribución de Laplace, y la denotamos $Laplace(x | \mu, \sigma)$ como la distribución que tiene función de densidad
    \[f(x | \mu, \sigma) = \frac{1}{2 \sigma} e^{-|x - \mu| / \sigma}, \ x \in \mathbb{R}.\]
\end{definition}

\begin{figure}[H]
    \begin{tikzpicture}[
         declare function={gauss(\x,\mu,\sigma)=1/(\sigma*sqrt(2*pi))*exp(-((\x-\mu)^2)/(2*\sigma^2));},
         declare function={laplace(\x,\mu,\sigma) = exp(-abs(\x-\mu) / \sigma)/(2*\sigma);}
        ]
        \begin{axis}[
            domain=-5:5,
            samples=200,
            axis lines=left,
            every axis y label/.style={at=(current axis.above origin),anchor=east},
            every axis x label/.style={at=(current axis.right of origin),anchor=north},
            height=6cm, width=12cm,
            enlargelimits=false,
            clip=false,
            axis on top,
            grid = major,
            legend pos=outer north east
        ]
            \addplot [very thick,cyan!70!black] {laplace(x,0,1)};
            \addlegendentry{Laplace, $\mu = 0, q = 1$}
            \addplot [very thick,cyan!40!black] {laplace(x,0,2)};
            \addlegendentry{Laplace, $\mu = 0, q = 2$}
            \addplot [very thick,magenta!80!black] {gauss(x,0,1)};
            \addlegendentry{Normal, $\mu = 0, q = 1$}
        \end{axis}
    \end{tikzpicture}
    \caption{Densidad de la distribución de Laplace comparada con la densidad de la distribución normal.}
    \label{fig:laplace}
\end{figure}

\end{comment}

\subsubsection{Distribución T de Student}

\subsubsection{Distribución de Dirichlet}
		La distribución de Dirichlet es la generalización en multivariable de la distribución Beta. Comúnmente, se utilizan las funciones de Dirichlet como funciones a priori en estadística Bayesiana. Definimos la distribución de Dirichlet de orden $n \geq 2$ con parámetros $\alpha_1, ..., \alpha_n$ tiene una función de densidad de probabilidad

		\begin{equation*}
		f(x_1, .. x_n|\alpha_1, ...,\alpha_n) = \frac{\Gamma(\alpha_1 + ... + \alpha_n)}{\Gamma(\alpha_1) + ... + \Gamma(\alpha_n)} \prod_{i=1}^{n} {x_i ^ {\alpha_i - 1}}
		\end{equation*}

		definida en el simplex abierto de (n-1) dimensiones definido por:

		\begin{equation*}
		x_1, ... , x_n > 0
		\end{equation*}
		\begin{equation*}
		x_1 + ... + x_{n-1} < 1
		\end{equation*}
		\begin{equation*}
		x_n =  1 - x_1 - ... - x_{n-1}
		\end{equation*}

		\begin{thm}
			Sea $X = (X_1, ... , X_k) \sim Dirichlet(X|\alpha_1, ... , \alpha_k, \alpha_{k+1}) $ se tiene que para cualquier $k_1 < k$  se verifica que $X' =  (X_1, ... , X_{k_1}) \sim D(X'|\alpha_1, ... , \alpha_{k_1}, \alpha_{k_1 + 1}^{*}) $ con  $\alpha_{k+1}^{*} =\sum_{j=1}^{k_1} {\alpha_{j}}$
		\end{thm}

		\begin{proof}
			Pendiente
		\end{proof}


\pagebreak

\importsection{Estimacion.tex}

\pagebreak

\section{La familia exponencial}

    En esta sección estudiamos una amplia familia de distribuciones, denominada la familia exponencial. Veremos que gran parte de las distribuciones que hemos estudiado hasta el momento pertenecen a esta familia.

    \begin{definition}
        Una variable aleatoria se distribuye respecto de una \emph{familia exponencial} si su función de distribución es de la forma
        \begin{equation} \label{eq:exponencial}
            f(x | \theta) = h(x) \exp\left(\sum^k_{i=1} \theta_i T_i(x)  + \Psi (\theta)\right),
        \end{equation}
        donde $\theta = (\theta_1, \ldots, \theta_k)$ y $h(x) \ge 0$, $\Psi(\theta)$, $T_1(x), \ldots, T_k(x)$ son funciones reales.
    \end{definition}

    Las familias exponenciales presentan características matemáticas y estadísticas muy convenientes. De estas características cabe destacar el siguiente resultado, que utiliza estadísticos suficientes introducidos en la Sección \ref{sec:estimacion:tge:sufi}.

    \begin{prop} \label{prop:exp:sufi}
        Sea $\{f(X | \theta): \theta \in \Theta\}$ una familia exponencial y sea una muestra $\utilde{X} = (X_1, \ldots, X_n)$. Entonces, $T(X) = (\sum_{j = 1}^n T_i(X_j))_{i = 1, \ldots, k}$ es un estadístico suficiente de dimensión $k$.
    \end{prop}
    \begin{proof}
        En efecto, utilizando \eqref{eq:exponencial} basta escribir $f(\utilde{x} | \theta)$ como sigue
        \begin{equation*}
            f(\utilde{x} | \theta) = \prod_{j=1}^n h(x_j) \exp\left(\sum^n_{j=1}\sum^k_{i=1} \theta_i T_i(x_j)  + n\Psi(\theta)\right) = \prod_{j=1}^{n}h(x_j) \exp\left(\sum_{i=1}^{k}\theta_i \sum^n_{j=1} T_i(x_j)  + n\Psi(\theta)\right). \qedhere
        \end{equation*}
    \end{proof}

    Nótese que la dimensión del estadístico suficiente encontrado no depende de la muestra. A continuación mostramos algunos ejemplos de familiais exponenciales.

    \begin{ex}[Distribución binomial] \label{ex:exp:binom}
        La función masa de probabilidad de una distribución binomial con $n$ fijo puede escribirse como sigue
        \[f(x|p) = \binom{n}{x} p^x (1-p)^{n-x} = \binom{n}{x} \exp(x\log(p) + (n-x) \log(1-p)) = \binom{n}{x} \exp(x\log(\frac{p}{1-p}) + n \log(1-p)). \]
        La aplicación  $f(p) = \log(\frac{p}{1-p}) = \log(\frac{1}{1-p} - 1)$ es una biyección de $(0,1)$ a $\mathbb{R}$. En este punto hacemos el cambio de variable $\theta = f(p)$.
        Hemos obtenido que la distribución binomial es una familia exponencial de parámetro $\theta$ con $h(x) = \binom{n}{x}$, $T_1(x) = x$ y $\Psi(\theta) = n \log(1 - f^{-1}(\theta))$. Según la Proposición \ref{prop:exp:sufi} un estadístico suficiente es $T(\utilde{X}) = \sum_{i = 1}^n X_i$ y, por tanto, la media muestral, $T(\utilde{X}) = \overline{X}$, es otro estadístico suficiente.
    \end{ex}

    En el ejemplo anterior hemos tenido que realizar un cambio de variable del espacio paramétrico para poder escribir la distribución de Bernoulli como una familia exponencial. El nuevo espacio paramétrico obtenido es el \emph{espacio paramétrico natural} de la familia. Para evitar trabajar con cambios de variables algunos autores definen las familias exponenciales como aquellas cuya función de desidad se puede escribir de la forma

    \begin{equation} \label{eq:exponencial:2}
        f(x | \theta) = h(x) \exp\left(\sum^k_{i=1} w_i(\theta) T_i(x)  + \Psi(\theta)\right),
    \end{equation}
    donde  $h(x) \ge 0$, $\Psi(\theta)$, $w_1(\theta), \ldots, w_k(\theta)$ y $t_1(x), \ldots, T_k(x)$ son funciones reales. En el Ejemplo \ref{ex:exp:binom} se tendría $w_1(p) = \log(\frac{p}{1-p})$ y $\Psi(p) = n \log(1-p)$.

    \begin{ex}[Distribución normal] \label{ex:exp:normal}
        La función de densidad de la distribución normal se puede escribir de la forma
        \[f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(- \frac{(x-\mu)^2}{2\sigma^2}\right) = \frac{1}{\sqrt{2 \pi} \sigma} \exp\left(- \frac{x^2}{2\sigma^2} + \frac{x\mu}{\sigma^2} - \frac{\mu^2}{2\sigma^2}\right)\]
        y, por tanto, es una familia exponencial con $h(x) = 1$, $\Psi(\mu, \sigma^2) = -\frac{\mu^2}{2\sigma^2} - \log(\sqrt{2\pi} \sigma)$, $T_1(x) = x$ y $T_2(x) = - x^2 / 2$.
        El espacio paramétrico natural se corresponde con $(1/\sigma^2, \mu/\sigma^2)$. No obstante, utilizamos los parámetros $(\mu, \sigma^2)$ debido a la interpretación estadística de los mismos.

        Como consecuencia de la Proposición \ref{prop:exp:sufi} obtenemos que cualquier variable aleatoria siguiendo una distribución $N(x|\mu, \sigma^2)$ verifica que $T(\utilde{X}) = (\sum_{i= 1}^n X_i, \sum_{i = 1}^n X_i^2)$ es un estadístico suficiente.
    \end{ex}

    La mayoría de las distribuciones estudiadas hasta el momento forman una familia exponencial. La Tabla \ref{table:exponencial} muestra una lista de ejemplos. No obstante, no toda familia de distribuciones es exponencial, como sucede con las distribuciones uniformes.

    \begin{table}[H]
    	\begin{center}
    		\begin{tabular}{|l|l|l|}
    			\hline
    			DENSIDAD & NOTACIÓN & SOPORTE\\
    			\hline \hline
                $\frac{1}{\sqrt{2 \pi} \sigma} \exp\left(- \frac{(x-\mu)^2}{2\sigma^2}\right)$ & $N(x|\mu, \sigma^2)$ & $\mathbb{R}$ \\
                \hline
                $\frac{1}{\Gamma(\alpha)\beta^\alpha}x^{\alpha - 1}e^{\frac{-x}{\beta}}$ & $ gamma(x|\alpha,\beta)$ &   $(0,\infty)  $\\ \hline
    			$\frac{1}{\Gamma(\frac{p}{2})2^{\frac{p}{2}}}x^{\frac{p}{2} - 1}e^{\frac{-x}{2}}$ & $\chi^2$ con p grados de libertad  & $(0,\infty)  $ \\ \hline
    			$\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha) \Gamma(\beta)} x ^{\alpha - 1}(1-x)^{\beta - 1}$ &  $beta(x| \alpha, \beta)$   &   $(0,1)$   \\ \hline
    			$\binom{n}{x} \theta ^x (1-\theta)^{n-x}$ &  $B(x|\theta, n)$ & ${0,1, \ldots , n}$   \\ \hline
    			$\frac{e^{-\lambda} \lambda^x}{x!}$ & $P(x|\lambda)$ & ${0,1, \ldots , n}$   \\ \hline
    		\end{tabular}
    	\end{center}
        \caption{Ejemplos de familias exponenciales.}
        \label{table:exponencial}
    \end{table}

\pagebreak
\importsection{Hipotesis.tex}

\pagebreak

\section{Estadística bayesiana} \label{sec:bayes}

En esta sección estudiaremos el modelo de inferencia estadística desde el punto de vista bayesiano. La estadística bayesiana proporciona resultados similares a la estadística clásica en el problema de estimación. Las ventajas de los modelos bayesianos serán remarcables al realizar tests de hipótesis, donde nos permitirán hablar de la probabilidad de que una hipótesis sea cierta o no. Recordemos que esta afirmación es imposible en la inferencia clásica ya que los parámetros de un modelo no son variables aleatorias.

\subsection{Introducción}

En primer lugar recordamos uno de los teoremas clásicos de la probabilidad, el Teorema de Bayes, que es la base de la inferencia Bayesiana. Supongamos que en el espacio de probabilidad $(\Omega,\mathcal{A},P)$ tenemos una partición de $\Omega$ dada por los sucesos $A_1,\dots,A_n$, todos ellos con probabilidad no nula. Sea $B$ un suceso no nulo del que conocemos sus probabilidades condicionadas a cada suceso $A_i$. Entonces, utilizando la definición de probabilidad condicionada obtenemos la probabilidad de cada $A_i$ condicionada al suceso $B$ como sigue

\begin{equation} \label{eq:condicionada}
	P(A_i|B)=\frac{P(A_i\cap B)}{P(B)}=\frac{P(B|A_i)P(A_i)}{P(B)}.
\end{equation}

A su vez, la ley de la probabilidad total establece que $P(B)=\sum_{i=1}^n{P(B|A_i)P(A_i)}$ y, por tanto, aplicando esta a igualdad a \eqref{eq:condicionada} deducimos
\begin{equation} \label{eq:bayes}
	P(A_i|B)=\frac{P(B|A_i)P(A_i)}{\sum_{i=1}^n{P(B|A_i)P(A_i)}},
\end{equation}
donde los valores $P(B|A_i)$ eran conocidos. La ecuación \eqref{eq:bayes} se conoce como Teorema de Bayes. A los valores $P(A_i)$ los llamamos \emph{probabilidades a priori}, mientras que a los valores $P(A_i|B)$ los denominamos las \emph{probabilidades a posteriori}. El Teorema de Bayes se puede deducir de igual forma para distribuciones de probabilidad.

La estadística bayesiana se basa en la interpretación subjetiva de la probabilidad. Utiliza la percepción existente por parte del investigador para otorgar una credibilidad a cada parámetro del modelo en forma de una distribución de probabilidad (distribución a priori). Posteriormente aplica el Teorema de Bayes para obtener una distribución de los parámetros condicionada a la muestra (distribución a posteriori), con la que formular inferencias con respecto al parámetro de interés.

Consideremos un problema de inferencia estadística en el que las observaciones se toman de una variable aleatoria $X$ que sigue una distribución $f(x|\theta)$, con $\theta\in\Theta$. Disponemos de información previa sobre $\theta$, que podemos recoger definiendo una distribución de probabilidad sobre el espacio $\Theta$, la distribución a priori, dando así a $\theta$ el carácter de variable aleatoria, con la peculiaridad de que no es observable. Sin embargo, sí observamos la variable aleatoria $X$ condicionada al verdadero valor que toma $\theta$, que llamaremos $\theta_0$. Así, el estudio de las observaciones de $X$ aporta información sobre el valor de $\theta$, información que debemos combinar con la distribución a priori para modificarla. El resultado de esta modificación es de nuevo una distribución sobre $\Theta$, que llamaremos la distribución a posteriori de $\theta$, una vez observada la variable aleatoria $X$. Estos son los planteamientos básicos que conforman el enfoque bayesiano de la estadística.

En lo que sigue definiremos formalmente la distribución a posteriori, recordando los conceptos que sean necesarios.

\begin{definition}
	Sea $X$ una variable aleatoria que sigue una distribución $f(x|\theta)$, con $\theta \in \Theta$. A una distribución $\pi(\theta)$ sobre el espacio $\Theta $ establecida con información previa conocida sobre $\theta$ se le llama distribución a priori de la variable aleatoria $\theta$.
	%Sea $X_{\sim}=(X_1,\dots,X_n)$ un vector de variables aleatorias de la familia de densidades $\{f(x_{\sim}|\theta)|\theta = (\theta_1,\dots,\theta_k) \in \Theta \subset \mathbb{R}^k \}$. A una distribución $\pi(\theta)$ sobre el espacio $\Theta $ establecida con información previa conocida sobre $\theta$ se le llama distribución a priori de la variable aleatoria $\theta$.
\end{definition}


\begin{remark}
	Dada una muestra aleatoria simple $\utilde{X} = (X_1,\dots,X_n)$ de $X$ y una distribución a priori $\pi(\theta)$, podemos calcular la distribución conjunta de $\utilde{X}$ y $\theta$ utilizando la definición de condicionamiento \cite{loeve}, que es el análogo a \eqref{eq:condicionada} para variables aleatorias. En efecto, en este caso obtenemos que la ditribución conjunta tiene la función de densidad
    \[f(\utilde{x},\theta)=f(\utilde{x}|\theta)\pi(\theta).\]

    A partir de la distribución conjunta podemos calcular la distribución marginal de $\utilde{X}$, que denotamos $m(\utilde{X})$. Esta distribución tiene función de densidad
	\[m(\utilde{x}) = \int_{\Theta}{f(\utilde{x},\theta)d\theta} = \int_{\Theta}{f(\utilde{x}|\theta)\pi(\theta)d\theta}.\]
\end{remark}

\begin{definition}
	%Dada una muestra $x_{\sim} = (x_1,\dots,x_n)$ de $X_{\sim}$, se define la probabilidad a posteriori de $\theta$ condicionada a la muestra como:

	Dada una muestra aleatoria simple $\utilde{X} = (X_1,\dots,X_n)$ de $X$, una realización de la muestra $\utilde{x}=(x_1,\dots,x_n)$ y una distribución a priori $\pi(\theta)$, se define la distribución a posteriori de $\theta$ como la ley de $\theta$ condicionada a $\utilde{X} = \utilde{x}$. La denotamos $\pi(\theta | \utilde{x})$.
\end{definition}

La función de densidad de la distribución a posteriori se puede calcular a partir de la distribución a priori y $f(\utilde{x}|\theta)$ utilizando de nuevo la definición de condicionamiento. En efecto, tenemos que
\[\pi(\theta|\utilde{x}) m(\utilde{x}) = f(\utilde{x}, \theta) = f(\utilde{x}|\theta)\pi(\theta).\]
En consecuencia, podemos expresar la distribución a posteriori de esta forma
\[\pi(\theta|\utilde{x}) = \frac{f(\utilde{x},\theta)}{m(\utilde{x})}.\]
A veces es conveniente omitir el uso de la distribución marginal $m(\utilde{x})$, en cuyo caso escribimos
\[\pi(\theta|\utilde{x})= \frac{f(\utilde{x}|\theta)\pi(\theta)}{\int_{\Theta}{f(\utilde{x}|\theta)\pi(\theta)\,d\theta}}.\]

%La distribución a posteriori, como acabamos de indicar, es una distribución condicional a las observaciones dadas. Como tal, es el cociente entre una densidad conjunta y una marginal. %Además, se evalúa sobre la ley conjunta o verosimilitud de la muestra. Pasamos a recordar estos conceptos.

Para finalizar esta sección, cabe destacar es que es posible no exigirle a la distribución de probabilidad a priori que integre, es decir, podrían distribuir una probabilidad infinita sobre $\Theta$. En tal caso se dice que la distribución es \textit{impropia}. Pese a su carácter impropio estas distribuciones nos pueden permitir hacer inferencias correctas.

\subsection{Estadística clásica vs bayesiana}

Veamos ahora las diferencias entre la inferencia clásica y la bayesiana. En la inferencia clásica destacan las siguientes características:

\begin{itemize}
	\item El concepto de probabilidad está limitado a aquellos sucesos en los que se pueden definir frecuencias relativas.
	\item $\theta$ es un valor fijo, pero desconocido.
	\item Se usa el concepto de intervalo de confianza.% (AÑADIR SI ESO)
	\item El método de muestreo es muy importante.
	\item Se pueden usar estimadores de máxima verosimilitud o estimadores insesgados.
    \item Los tests de hipótesis se construyen fijando un tamaño $\alpha$ y minimizando los errores de tipo 2.
\end{itemize}

Por su parte, en la inferencia bayesiana destacan:

\begin{itemize}
	\item Podemos establecer probabilidades previas para cualquier suceso.
	\item $\theta$ es una variable que sigue una distribución de probabilidad.
	\item Se usa el concepto de intervalo de credibilidad para $\theta$.% (AÑADIR SI ESO)
	\item El método de muestreo no importa; solo importan los datos.
	\item Se utilizan estimadores diferentes según la utilidad; la estimación es un problema de decisión.
    \item En tests de hipótesis se puede hablar de probabilidad de que una hipótesis sea cierta.
\end{itemize}

Uno de los aspectos más criticados de la estadística bayesiana es el grado de subjetividad a la que se expone la inferencia por el hecho de que es el experimentador quien define la distribución a priori. En cualquier caso, en lo que hay coincidencia es en que si hay información sobre $\theta$, entonces ésta tiene que ser utilizada en la inferencia.

\ \newline
Como acabamos de decir, una parte muy importante en la inferencia bayesiana es la selección de la distribución a priori. En muchos casos, si no disponemos de una distribución clara para modelar $\theta$ es posible considerar distribuciones específicas que permitan simplificar los cálculos de la distribución a posteriori. A continuación estudiaremos distintos medios para seleccionar estas distribuciones.

\subsection{Familias conjugadas}

La principal dificultad que surge en los problemas de inferencia bajo la perspectiva bayesiana es tanto la confianza que se pueda esperar de la distribución a priori como el cálculo de la distribución a posteriori. La primera cuestión es importante ya que la inferencia que se realice puede depender de la elección de la distribución inicial, razón por la cual en muchos casos se recurre a distribuciones no informativas, que no imponen unas condiciones muy fuertes sobre el parámetro. Otra tendencia en la elección de las distribuciones a priori es aprovechar la información que proporciona la muestra para mejorar la distribución inicial.% dando origen a las denominadas distribuciones intrínsecas a priori, de gran auge en la actualidad.

En cuanto al cálculo de la distribución a posteriori, no todas las distribuciones a priori conducen a cómputos asequibles ni a una distribución tratable y, en ocasiones, hay que recurrir a métodos numéricos para poder trabajar con ellas. Por tanto, es deseable obtener distribuciones a priori que nos faciliten este proceso.

En esta sección nos centramos en este último problema. Buscamos familias de distribuciones a priori cuyas distribuciones a posteriori asociadas sean de fácil cálculo. En este sentido surge el concepto de familias a priori conjugadas.

\begin{definition}
	Sea $\mathcal{F} = \{\pi_i(\theta): i\in I\}$ una familia de distribuciones a priori. Se dice que $\mathcal{F}$ es conjugada respecto de la familia de densidades $\mathcal{P} = \{f(x|\theta): \theta\in\Theta\}$ si para cualquier $\pi(\theta)\in\mathcal{F}$ y $f(x|\theta)\in \mathcal{P}$ se verifica que $\pi(\theta|x) \in \mathcal{F}$. Es decir, una familia $\mathcal{F}$ de distribuciones a priori es conjugada respecto a la familia dada si y solo si las distribuciones a posteriori pertenecen de nuevo a $\mathcal{F}$.
\end{definition}

Recordemos que $\pi(\theta|x) = f(x|\theta)\pi(\theta) / m(x)$. El denominador $m(x)$ es una constante ya que $x$ está fijo. Como con secuencia, obtenemos la siguiente observación.

\begin{remark}
	Sean $\pi(\theta), \Pi(\theta) \in \mathcal{F}$. Se tienen las siguientes condiciones equivalentes:
	\begin{enumerate}%[label=\roman)*]
		\item $f(x|\theta)\pi(\theta) \propto \Pi(\theta)$;
		\item $\pi(\theta|x)=\Pi(\theta)$.
	\end{enumerate}
\end{remark}

 Por tanto, en la definición de familias conjugadas, la afirmación $\pi(\theta|x) \in \mathcal{F}$ equivale a decir que $f(x|\theta)\pi(\theta) \propto \Pi(\theta)$ para cierta distribución $\Pi(\theta) \in \mathcal{F}$. Este hecho se enuncia en la siguiente proposición.

\begin{prop}
    Una familia de distribuciones a priori $\mathcal{F} = \{\pi_i(\theta): i\in I\}$ es conjugada respecto de la familia de densidades $\mathcal{P} = \{f(x|\theta): \theta\in\Theta\}$ si, y solo si, el producto de cualesquiera dos distribuciones de ambas familias vuelve a ser, salvo constante, una distribución de la familia de distribuciones a priori.
\end{prop}

Tener una familia de distribuciones conjugadas a priori nos permite simplificar en gran medida el cálculo de la distribución a posteriori. En efecto, solo tenemos que identificar la distribución de $\mathcal{F}$ que es proporcional a $f(x|\theta)\pi(\theta)$. Esa distribución coincidirá con $\pi(\theta | x)$. De esta forma evitamos tener que realizar el cálculo de la distribución marginal de $x$, que suele hacerse mediante una integral. Además, en caso de necesitar el valor $m(x)$ basta darse cuenta de que $m(x) = f(x|\theta)\pi(\theta) / \pi(\theta|x)$.

%En el denominador del cociente tenemos, salvo constantes, una integral de una función de la familia $\mathcal{F}$, que sabemos que integra 1. Las constantes son las mismas en numerador y denominador, dando lugar así a una distribución a posteriori de la misma familia.

Es posible calcular las distribuciones conjugadas para las familias de distribuciones clásicas, obteniendo de nuevo otras distribuciones clásicas.

\begin{prop} ~\\
    \vspace*{-6mm}
	\begin{itemize}
		\item La familia de distribuciones Beta es una familia de distribuciones conjugada para las distribuciones de Bernouilli, binomiales y binomiales negativas.

		\item La familia de distribuciones Gamma es una familia de distribuciones conjugada para las distribuciones de Poisson.% y (exponenciales (definir)).

		\item La familia de distribuciones normales es una familia de distribuciones conjugada para la familia de distribuciones normales con varianza conocida.

		\item La familia de distribuciones de Dirichlet es una familia de distribuciones conjugada para la familia de distribuciones multinomiales.
	\end{itemize}
\end{prop}

Veamos algún ejemplo de los proporcionados por la proposición anterior.

\begin{ex}
	 Vamos a considerar una distribución de Poisson de parámetro $\lambda > 0$ y, como distribución a priori, una Gamma de parámetros $\alpha$ y $\beta$. Dada una muestra $\utilde{x} = (x_1,\dots,x_n)$ se tiene
	\[f(\utilde{x}|\lambda) = \frac{e^{-n\lambda}\lambda^{\sum{x_i}}}{\prod{x_i!}} \text{ y } \pi(\lambda)=\frac{\beta^{\alpha}\lambda^{\alpha-1}e^{-\beta\lambda}}{\Gamma(\alpha)}.\]
    Multiplicando ambas distribuciones obtenemos
	\[f(\utilde{x}|\lambda)\pi(\lambda)=\frac{e^{-n\lambda}\lambda^{\sum{x_i}}\lambda^{\alpha-1}e^{-\beta\lambda}\beta^\alpha}{\prod{x_i!}\Gamma(\alpha)}\]
	\[=\frac{\beta^{\alpha}}{\prod{x_i!}\Gamma(\alpha)}\lambda^{\sum{x_i}+\alpha-1}e^{-(\beta+n)}\propto Gamma\left(\lambda|\alpha+\sum{x_i},\beta+n\right).\]

	Es decir, para variables aleatorias de Poisson de parámetro $\lambda$, escogiendo una distribución a priori Gamma de parámetros $\alpha$ y $\beta$ obtenemos como distribución de $\lambda$ a posteriori una nueva Gamma, esta vez de parámetros $\alpha + \sum_{i = 1}^n{x_i}$ y $\beta+n$, donde $n$ es el tamaño de la muestra.

	Notemos que la conjugación nos ha permitido evitar el cálculo de la distribución marginal de $x$. Si optamos por calcularla, obtendríamos:

	[PROXIMAMENTE]

	Llegando de nuevo al mismo resultado.
\end{ex}

\begin{ex}
	Consideremos ahora una distribución multinomial dada por la función masa de probabilidad
	\[f(x_1,\dots,x_{k}|\theta_1,\dots,\theta_{k}) = \frac{n!}{x_1!\dots x_k!}\theta_1^{x_1}\dots \theta_k^{x_k},\]
	con $\sum_{i=1}^{k}{x_i} = n$, $0 < \theta_i < 1$ y $\sum_{i=1}^{k}{\theta_i}=1$.

	Llamamos $\theta = (\theta_1,\dots,\theta_k)$. Elegimos como distribución a priori la distribución de Dirichlet de parámetros $\alpha_1,\dots,\alpha_k$, cuya función de densidad viene dada por
    \[\pi(\theta) \propto \frac{\Gamma(\alpha_1+\dots+\alpha_k)}{\Gamma(\alpha_1)\dots\Gamma(\alpha_k)}\prod_{i=1}^{k}{\theta_i^{\alpha_i-1}}.\]
	Entonces, dada una muestra $\utilde{x}=(x_1,\dots,x_k)$, se tiene
	\[\pi(\theta|\utilde{x}) \propto \pi(\theta)f(\utilde{x}|\theta)
	\propto \left(\prod_{i=1}^{k}{\theta_i^{\alpha_i-1}}\right)\left(\prod_{i=1}^{k}{\theta_i^{x_i}}\right)\]
	\[ = \prod_{i=1}^{k}{\theta_i^{x_i+\alpha_i-1}} \propto Dirichlet(\theta|\alpha_1+x_1,\dots,\alpha_k+x_k), \]
    obteniendo que la distribución a posteriori sigue una Dirichlet.
\end{ex}

\begin{ex}

Consideremos $X \sim \mathcal{N}(\mu,\sigma^2)$, con $\sigma^2$ conocido. Fijamos una muestra $\utilde{x}=(x_1,\dots,x_n)$. La función de densidad está condicionada únicamente a $\mu$. Tenemos que
%\[f(x|\mu) = (2\pi\sigma^2)^{-1/2}e^{-\frac{(x-\mu)^2}{2\sigma^2}}.\]
\[f(\utilde{x}|\mu) = (2\pi\sigma^2)^{-n/2}\exp\left(-\sum{\frac{(x_i-\mu)^2}{2\sigma^2}}\right).\]

Elegimos una distribución a priori $\mu \sim \mathcal{N}(\eta,\tau^2)$, es decir,
\[\pi(\mu) = (2\pi\tau^2)^{-1/2} \exp\left(-\frac{(\mu-\eta)^2}{2\tau^2}\right).\]

Calculamos la distribución conjunta, obteniendo
\[f(\utilde{x}|\mu)\pi(\mu) \propto \exp{\left(-\sum{\frac{(x_i-\mu)^2}{2\sigma^2}-\frac{(\mu-\eta)^2}{2\tau^2}}\right)}.\]

Llamamos $\overline{x}=\frac{1}{n}\sum{x_i}$ y $s^2 = \frac{1}{n}\sum{(xb_i-\overline{x})^2}$. Notemos que ninguna de las dos expresiones depende de $\mu$. Usando que $\sum{(x_i-\mu)^2} = \sum{(x_i-\overline{x} + \overline{x} -\mu)^2} = \sum{(x_i-\overline{x})^2}+2(\overline{x}-\mu)\sum{(x_i-\overline{x})} + \sum{(\overline{x}-\mu)^2} = ns^2+ n(\overline{x}-\mu)^2$, tenemos
\[f(\utilde{x}|\mu)\pi(\mu) \propto \exp{\left(-\frac{n}{2\sigma^2}[s^2+(\overline{x}-\mu)^2]-\frac{(\mu-\eta)^2}{2\tau^2}\right)}\]
\[=
\exp{\left(-\frac{ns^2}{2\sigma^2}\right)}\exp{\left(-\frac{1}{2\sigma^2\tau^2}[n\tau^2(\overline{x}-\mu)^2+\sigma^2(\mu-\eta)^2]\right)}
\propto
 \exp{\left(-\frac{1}{2\sigma^2\tau^2}[n\tau^2(\overline{x}-\mu)^2+\sigma^2(\mu-\eta)^2]\right)}.\]

 Ahora, desarrollamos la expresión $n\tau^2(\overline{x}-\mu)^2+\sigma^2(\mu-\eta)^2$. Podemos separarla según los sumandos que dependan o no de $\mu$. A continuación, dividimos la exponencial en dos partes, una con los sumandos independientes de $\mu$ y otra con los dependientes. La expresión resultante es
 \[f(\utilde{x}|\mu)\pi(\mu) \propto
 \exp{\left(-\frac{n\overline{x}^2\tau^2+\sigma^2\eta^2}{2\sigma^2\tau^2}\right)}
 \exp{\left(-\frac{\mu^2(n\tau^2+\sigma^2)-2\mu(n\overline{x}\tau^2+\sigma^2\eta)}{2\sigma^2\tau^2}\right)}.\]
 \[\propto
 \exp{\left(-\frac{1}{2\sigma^2\tau^2}\left[\mu^2(n\tau^2+\sigma^2)-2\mu(n\overline{x}\tau^2+\sigma^2\eta)\right]\right)}.
 \]

 Ahora ajustamos cuadrados sobre el exponente. Procedemos como sigue
 \[-\frac{1}{2\sigma^2\tau^2}\left[\mu^2(n\tau^2+\sigma^2)-2\mu(n\overline{x}\tau^2+\sigma^2\eta)\right]
 =
 -\frac{n\tau^2+\sigma^2}{2\sigma^2\tau^2}\left[\mu^2-2\mu\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}\right]
 \]
 \[=
 -\frac{n\tau^2+\sigma^2}{2\sigma^2\tau^2}\left[\mu^2-2\mu\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}+\left(\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}\right)^2\right] + \frac{n\tau^2+\sigma^2}{2\sigma^2\tau^2}\left(\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}\right)^2
 \]
 \[=
  -\frac{n\tau^2+\sigma^2}{2\sigma^2\tau^2}\left[\mu-\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}\right]^2 + \frac{\left(n\overline{x}\tau^2+\sigma^2\eta\right)^2}{2\sigma^2\tau^2\left(n\tau^2+\sigma^2\right)}.\]

Volviendo a la distribución conjunta, hemos obtenido
\[f(\utilde{x}|\mu)\pi(\mu) \propto
\exp{\left(\frac{\left(n\overline{x}\tau^2+\sigma^2\eta\right)^2}{2\sigma^2\tau^2\left(n\tau^2+\sigma^2\right)}\right)}
\exp{\left(-\frac{n\tau^2+\sigma^2}{2\sigma^2\tau^2}\left[\mu-\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}\right]^2\right)}
\]
\[ \propto
\exp{\left(-\frac{n\tau^2+\sigma^2}{2\sigma^2\tau^2}\left[\mu-\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}\right]^2\right)}
=
\exp{\left(-\left[\mu-\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2}\right]^2 / \left[2\frac{\sigma^2\tau^2}{n\tau^2+\sigma^2}\right]\right)}
\]
\[
\propto \mathcal{N}\left(\mu\left|\frac{n\overline{x}\tau^2+\sigma^2\eta}{n\tau^2+\sigma^2},\frac{\sigma^2\tau^2}{n\tau^2+\sigma^2}\right.\right),
\]

obteniendo finalmente una distribución a posteriori también normal. Hemos demostrado que la familia normal es conjugada.
\end{ex}


\subsection{Distribuciones objetivas. Distribución de Jeffreys}

\begin{definition}
    Sea $\{f(x | \theta): \theta \in \Theta \}$ una familia de distribuciones con parámetro $\theta \in \Theta$. La distribución a priori de Jeffreys se define como $\pi^{J}(\theta) \propto \sqrt{\mathcal{I}_X(\theta)}$.
\end{definition}

\begin{ex}
    Vamos a estudiar la distribución a priori de Jeffreys para la distribución binomial. En el ejemplo \ref{ex:fisher:binom} se calculó la función de información de Fisher para la distribución binomial. A partir de los resultados obtenidos tenemos que $\pi^J(\theta) \propto \theta^{-1/2} (1 - \theta)\theta^{-1/2}$. Por tanto, $\pi^J(\theta)$ sigue una distribución $beta(1/2,1/2)$. La distribución a posteriori para $x = (x_1, \ldots, x_k)$ viene dada por
    \[\pi(\theta; x) \propto \pi(\theta) \prod_{i = 1}^k f(x_i; \theta) \propto \theta^{\sum x_i -1/2} (1 - \theta)^{\sum (n-x_i) -1/2},\]
    esto es, $\pi(\theta;x)$ sigue una distribución $beta(k\overline{x} -1/2, k(n - \overline{x}) - 1/2)$. Recordando el Corolario \ref{cor:beta:esp} podemos calcular la esperanza y la varianza de la distribución a posteriori:
    \[E[\pi(\theta; x)] = \frac{k\overline{x} -1/2}{kn - 1} = \frac{\overline{x} -1/(2k)}{n - 1/k};\]
    \[Var(\pi(\theta; x)) = \frac{(k\overline{x} -1/2)(k(n - \overline{x}) - 1/2)}{(kn - 1)^2(kn)} = \frac{(\overline{x} -1/{2k})((n - \overline{x}) - 1/(2k))}{kn(n - 1/k)^2}.\]
    Para $k$ lo suficientemente grande $E[\pi(\theta; x)] \approx \overline{x} / n$, que es el estimador máximo verosímil. Por tanto, cuando $k \to \infty$ obtenemos que $Var(\pi(\theta; x)) \to 0$ y $E[\pi(\theta; x)] \to \theta_0$.
\end{ex}

Notemos que la distribución de Jeffreys podría ser impropia, es decir, no integrable. Veámoslo con el siguiente ejemplo:

\begin{ex} \label{ex:jeff:poisson}

	Consideramos la familia de distribuciones de Poisson con parámetro $\lambda$, con funciones de densidad $f(x|\lambda) = \frac{e^{-\lambda} \lambda ^{x}}{x!}$, con $\lambda > 0$ y $x\in\mathbb{N}\cup\{0\}$. Recordemos que si $X \sim f(x|\lambda)$, entonces $I_X(\lambda) = \frac{1}{\lambda}$, y en consecuencia la distribución de Jeffreys sería $\pi^{\mathcal{J}}(\lambda) \propto \lambda^{-\frac{1}{2}}$. Sin embargo, esta función no integra en $\mathbb{R}^{+}$, el dominio de $\lambda$, pues $\int_{0}^{\infty}{\lambda^{-\frac{1}{2}}d\lambda} = [2\lambda^{\frac{1}{2}}]_{0}^{\infty} = \infty$. Estamos por tanto ante una distribución a priori impropia. En esta situación (según el procedimiento de clase ¿?) normalizamos la distribución, tomando $\pi^{\mathcal{J}}(\lambda) = c\lambda^{-\frac{1}{2}}$, con $c > 0$ arbitrario. Si conseguimos que la distribución a posteriori no dependa de $c$ podremos utilizarla para hacer inferencia.

	Pasemos a calcular la distribución a priori. Consideramos una muestra aleatoria simple $X_1,\dots,X_n$ de $X$, y el vector de observaciones de la muestra $\utilde{x}=(x_1,\dots,x_n)$. Entonces $f(\utilde{x}|\lambda)=\prod_{i=1}^{n}{f(x_i|\lambda)}=\frac{e^{-n\lambda}\lambda^{\sum{x_i}}}{\prod{x_i!}}$. La distribución conjunta es

	\[f(\utilde{x}|\lambda)\pi^{\mathcal{J}}(\lambda) = \frac{c}{\prod{x_i}}e^{-n\lambda}\lambda^{\sum{x_i}-\frac{1}{2}}.\]

	Y la distribución marginal de $x$ es

	\[f(\utilde{x}) = \frac{c}{\prod_{i^=1}^{n}{x_i}} \int_{0}^{\infty} {e^{-n\lambda} \lambda^{\sum{x_i} - \frac{1}{2}} d\lambda} =  \left[\begin{array}{ll} y = n \lambda \\ dy = nd\lambda \end{array} \right]  =\]

	\[ = \frac{c}{n ^{\sum{x_i} + \frac{1}{2}}\prod_{i^=1}^{n}{x_i}} \int_{0}^{\infty} {e^{-y} y^{\sum{x_i}-\frac{1}{2} } dy } = \frac{c}{n ^{\sum{x_i} + \frac{1}{2}}\prod{x_i!}}\Gamma(\sum{x_i} + \frac{1}{2}).\]

	Podemos comprobar que ambas distribuciones están indeterminadas por $c$, y aun así podemos obtener la distirbución a priori,

	\[\pi(\lambda|\utilde{x}) = \frac{ \frac{c}{\prod{x_i!}}e^{-n\lambda}\lambda^{\sum{x_i}-\frac{1}{2}} }{\frac{c}{\prod{x_i!}} \frac{1}{n ^{\sum{x_i} + \frac{1}{2}}}\Gamma(\sum{x_i} + \frac{1}{2}) } = \frac{e^{-n\lambda} \lambda^{\sum{x_i} - \frac{1}{2}} n^{\sum{x_i} + \frac{1}{2}}}{\Gamma(\sum{x_i} + \frac{1}{2})}. \]

		Hemos visto por tanto que una distribución a priori impropia también nos permite realizar inferencias. Un último detalle que podemos observar es que $f(\utilde{x}) \ne \prod{f(x_i)}$. Esta es otra de las diferencias entre la estadística clásica y la bayesiana. En la bayesiana no se sigue la hipótesis de que los datos sean incondicionalmente independientes, si no que se supone que los datos son condicionalmente independientes respecto a $\lambda$ (o $\theta$ en general), que es lo que se asume al calcular las verosimilitudes.

\end{ex}


\begin{comment}


\begin{ex}
	Cálculo de marginales en una distribución de Poisson con $ f(x_i|\lambda) = \frac{e^{-\lambda} \lambda ^{x_i}}{x_i!}, \lambda > 0 $ y $ x_i = 0,1,2, ... $
	\\En primer lugar calculamos la distribución a priori de Jeffreys $\Pi^y (\theta) = I_X(\theta)^{\frac{1}{2}} $. Por tanto, para nuestra función de distribución  $\Pi^y (\theta) = - \lambda ^{-\frac{1}{2}} $, pero para que sea una distribución de probabilidad tendremos que normalizarla, para ello hacemos $\Pi^y (\theta) = - \lambda ^{-\frac{1}{2}} c $, ahora bien, esta función no se puede normalizar dado que $\int_{0}^{\infty} c \lambda^\frac{1}{2} d\lambda = \infty $

	No estoy segura de por qué pero esto se puede hacer. De todas formas, podemos calcular la distribución a posteriori, para ello consideramos $f(x|  \lambda) =  \frac{e^{-\lambda} \lambda ^{\sum{x_i}}} {\prod{x_i!}}  , x = (x_1, x_2, ...) $.

	Podemos calcular la distribución marginal de x, $f(x) = \frac{c}{\prod_{i^=1}^{n}{x_i}} \int_{0}^{\infty} {e^{-n\lambda} \lambda^{\sum{x_i} - \frac{1}{2}} d\lambda} =  \left [ y = n \lambda , dy = nd\lambda \right ]  = \frac{c}{n ^{\sum{x_i} + \frac{1}{2}}\prod_{i^=1}^{n}{x_i}} \int_{0}^{\infty} {e^{-y} y^{\sum{x_i}-\frac{1}{2} } dy } = \frac{c}{n ^{\sum{x_i} + \frac{1}{2}}}\Gamma(\sum{x_i} + \frac{1}{2}) $

	Como, $f(x) != \prod f(x_i)$, concluimos que las variables no son incondicionalmente independientes si no condicionalmente independientes.

	Podemos observar que la distribución marginal de x está indeterminada por $c$. A pesar de ello, podemos calcular la distribución a posteriori de la siguiente forma:
	$f(\lambda, x) = \frac{ \frac{c e^{-n \lambda} \lambda{\sum{x_i} - \frac{1}{2}} }{x_1! x_2! ... x_n!} }{\frac{c}{x_1! ... x_n!} \frac{1}{n \sum{x_i} + \frac{1}{2} \Gamma(\sum{x_i} + \frac{1}{2})}} = \frac{e^{-n\lambda} \lambda^{\sum{x_i} - \frac{1}{2}} n^{\sum{x_i} + \frac{1}{2}}}{\Gamma(\sum{x_i} + \frac{1}{2})}$
\end{ex}
\end{comment}


La distribución de Dirichlet es la generalización en multivariable de la distribución Beta. Comúnmente, se utilizan las funciones de Dirichlet como funciones a priori en estadística Bayesiana. Definimos la distribución de Dirichlet de orden $n \geq 2$ con parámetros $\alpha_1, ..., \alpha_n$ tiene una función de densidad de probabilidad

\begin{equation*}
f(x_1, .. x_n|\alpha_1, ...,\alpha_n) = \frac{\Gamma(\alpha_1 + ... + \alpha_n)}{\Gamma(\alpha_1) + ... + \Gamma(\alpha_n)} \prod_{i=1}^{n} {x_i ^ {\alpha_i - 1}}
\end{equation*}

definida en el simplex abierto de (n-1) dimensiones definido por:

\begin{equation*}
x_1, ... , x_n > 0
\end{equation*}
\begin{equation*}
x_1 + ... + x_{n-1} < 1
\end{equation*}
\begin{equation*}
x_n =  1 - x_1 - ... - x_{n-1}
\end{equation*}

\begin{thm}
	Sea $X = (X_1, ... , X_k) \sim Dirichlet(X|\alpha_1, ... , \alpha_k, \alpha_{k+1}) $ se tiene que para cualquier $k_1 < k$  se verifica que $X' =  (X_1, ... , X_{k_1}) \sim D(X'|\alpha_1, ... , \alpha_{k_1}, \alpha_{k_1 + 1}^{*}) $ con  $\alpha_{k+1}^{*} =\sum_{j=1}^{k_1} {\alpha_{j}}$
\end{thm}

\begin{proof}
	Pendiente
\end{proof}

\subsection{Convergencia de distribuciones a posteriori}

Nos planteamos en este punto la convergencia de las distribuciones a posteriori cuando el tamaño de las muestras crece.

Supongamos que queremos hacer inferencia sobre un fenómeno que sigue una distribución $f(x|\theta_0)$, con una medida de probabilidad $P_{\theta_0}$. Conocemos la familia de distribuciones $\{f(x|\theta)|\theta\in\Theta\}$. Queremos ver, dada una distribución a priori $\pi(\theta)$, si converge la distribución a posteriori $\pi(\theta|X_1,\dots,X_n)$, para las variables i.i.d. $X_1,\dots,X_n \sim f(x|\theta_0)$,  cuando $n \to \infty$.

En general, si el espacio paramétrico no es discreto, el estudio de la convergencia de la distribución marginal no es sencillo. Estudiaremos la convergencia de la distribución a posteriori cuando el espacio paramétrico es $\Theta = \{\theta_1,\dots,\theta_k\}$ discreto.


\begin{thm}
	En las condiciones anteriores, se tiene que

	\[\pi(\theta|X_1,\dots,X_n) \xrightarrow[n\to\infty]{P_{\theta_0}} \theta_0.\]
\end{thm}

\begin{proof}

	Supongamos $\Theta = \{\theta_1,\dots,\theta_k\}$. La distribución a priori viene determinada por las probabilidades de cada $\theta_j$ y vendrá dada por $\pi(\theta_j)=p_j$, con $p_j\in[0,1]$ y $\sum_{i=1}^{k}{p_i}=1$, para $j=1,\dots,k$. Podemos suponer también que $t\in\{1,\dots,k\}$ es el índice del parámetro que corresponde al verdadero valor, es decir, $\theta_t = \theta_0$.


	Consideramos las variables aleatorias i.i.d. $X_1,\dots,X_n$ con distribución $f(x|\theta_t)$ y una muestra $x_1,\dots,x_n$. La distribución a posteriori en este caso discreto vendrá dada, para cada $\theta_i$, por:

	\[\pi(\theta_i|x_1,\dots,x_n) = \frac{p_i \prod_{j=1}^n{f(x_j|\theta_i)}}{\sum_{r=1}^k{p_r\prod_{j=1}^n{f(x_j|\theta_r)}}}\]

	Multiplicando numerador y denominador por $\left(\prod_{j=1}^n{f(x_j|\theta_t)}\right)^{-1}$ obtenemos:

	\begin{equation} \label{eq:tcfd1}
		\pi(\theta_i|x_1,\dots,x_n) = \frac{p_i \prod_{j=1}^n{\frac{f(x_j|\theta_i)}{f(x_j|\theta_t)}}}{\sum_{r=1}^k{p_r\prod_{j=1}^n{\frac{f(x_j|\theta_r)}{f(x_j|\theta_t)}}}}
	\end{equation}


	Estudiemos la convergencia de $\prod_{j=1}^n{\frac{f(x_j|\theta_i)}{f(x_j|\theta_t)}}$. Si $i=t$, claramente tenemos que el resultado es 1. En caso contrario, tomando logaritmos, obtenemos:

	\begin{equation} \label{eq:tcfd2}
	\log{\prod_{j=1}^n{\frac{f(x_j|\theta_i)}{f(x_j|\theta_t)}}} = \sum_{j=1}^{n}{log{\frac{f(x_j|\theta_i)}{f(x_j|\theta_t)}}} = n\left(\frac{1}{n}\sum_{j=1}^{n}{log{\frac{f(x_j|\theta_i)}{f(x_j|\theta_t)}}}\right)
	\end{equation}

	Ahora, las variables aleatorias $Z_i \sim \log{\frac{f(x_j|\theta_i)}{f(x_j|\theta_t)}}$ son i.i.d, luego por las leyes de los grandes números el término $\frac{1}{n}\sum_{j=1}^{n}{log{\frac{f(x_j|\theta_i)}{f(x_j|\theta_t)}}}$ converge en probabilidad $P_{\theta_t}$ a la esperanza de cualquiera de ellas, $E\left[\log{\frac{f(x_j|\theta_i)}{f(x_j|\theta_t)}}\right]$. Además, como consecuencia de la desigualdad de la información (proposición \ref{prop:desigualdad}), dicha esperanza es un valor estrictamente negativo. En consecuencia, a partir de la expresión obtenida en (\ref{eq:tcfd2}), podemos concluir que:

	\[\log{\prod_{j=1}^n{\frac{f(x_j|\theta_i)}{f(x_j|\theta_t)}}} \xrightarrow[n\to\infty]{P_{\theta_t}} -\infty\]

	La continuidad del logaritmo nos asegura que $\prod_{j=1}^n{\frac{f(x_j|\theta_i)}{f(x_j|\theta_t)}} \xrightarrow[n\to\infty]{P_{\theta_t}} 0$.

	Finalmente, aplicando lo que acabamos de obtener a (\ref{eq:tcfd1}), concluimos que:

	\begin{itemize}
		\item Si $i \ne t$, $\pi(\theta_i|X_1,\dots,X_n) \xrightarrow[n\to\infty]{P_{\theta_t}} \frac{0}{p_t+\sum{0}} = 0$
		\item Si $i = t$, $\pi(\theta_i|X_1,\dots,X_n) \xrightarrow[n\to\infty]{P_{\theta_t}} \frac{p_t}{p_t+\sum{0}} = 1$
	\end{itemize}

	Pero esto es equivalente a decir que la distribución a posteriori degenera a $\theta_t$ en probabilidad $P_{\theta_t}$.
\end{proof}

\begin{remark}
	Observemos que en la convergencia de la distribución a posteriori no ha influido la distribución a priori escogida. Esto nos indica que cuando el tamaño de la muestra es grande, la distribución que hayamos elegido a priori no va a tener mucha influencia sobre la distribución que utilizaremos para realizar inferencia.
\end{remark}

\pagebreak

\section{Test de Hipótesis Bayesianos}

En este apartado vamos a realizar un estudio similar al que se hizo con los test de hipótesis clásicos, pero haciendo uso de las herramientas que nos proporciona la estadística bayesiana. Para ello, recordemos que si queremos considerar un modelo bayesiano, tenemos que dotarnos de una familia de densidades y de una distribución a priori, como se sigue : ¿$\mathcal{M} = \{\pi_i(\theta)|i\in I\, \pi(\theta)\}$?.

Entonces consideramos dos posibles situaciones, con las que obtenemos dos modelos $ M_1 : \{f(x | \theta_1, M_1), \pi(\theta_1, M_1) \}$ y $ M_2 : \{f(x | \theta_2, M_2), \pi(\theta_2,M _2) \}$

Ahora bien, sabiendo que $\pi(\theta_i, M_i) =  \pi(M_i) \pi(\theta_i|M_i)$  para $i = 1,2.$, lo aplicamos a nuestros dos modelos y obtenemos que $ M_1 : \{f(x | \theta_1, M_1), \pi(M_1) \pi(\theta_1|M_1)$ y $M_2 : \{f(x | \theta_2, M_2), \pi(M_2) * \pi(\theta_2|M_2)$.

Supongamos que tenemos una muestra (independiente e idénticamente distribuida) de tamaño n que proviene de alguno de los modelos, esto es $\utilde{x} = (x_1,...,x_n)$. Con ella ,se quiere calcular la probabilidad a posteriori del modelo $M_1$. Para ello, necesitamos tomar la verosimilitud de la muestra respecto de $\theta_i$ y $M_i (i = 1,2)$, es decir, $\prod_{i=1}^{n}{f(x_i|\theta_1,M_1)}$ y $\prod_{i=1}^{n}{f(x_i|\theta_2,M_2)}$.

Con ello, la probabilidad a posteriori del modelo $M_1$ se obtiene de la siguiente forma:

\[P(\theta_1,M_1| \utilde{x}) = \frac{\prod_{i=1}^{n}{f(x_i|\theta_1,M_1)}\pi(\theta_1,M_1)}{\pi(M_1)m(\utilde{x}|M_1)+\pi(M_2)m(\utilde{x}|M_2)} \]

\begin{comment}
$ P(\theta_1, M_1 | \utilde{x}) =  \[\frac{\prod_{i=1}^{n}{f(x_i|\theta_1,M_1)}}\ * \pi(\theta_1,M_1)}{m(\utilde{x} | M_1) * \pi(M_1) + m(\utilde{x} | M_2) * \pi(M_2)}\], donde $m(\utilde{x} | M_i) = \int_{\Theta_i}{f(\utilde{x} | \theta_i, M_i) * \pi(\theta_i | M_i)  d\theta_i}, para i = 1,2 $.
\end{comment}

Por tanto, la probabilidad del modelo $M_1$ condicionado a la muestra $\utilde{x}$ es $P(M_1 | \utilde{x}) = \frac{m(\utilde{x}| M_1) \pi(M_1) }{m(\utilde{x}| M_1)\pi(M_1) + m(\utilde{x}| M_2)\pi(M_2) }$.

Esta expresión se pude reescribir usando el factor de Bayes $B_{21}(\utilde{x}) = \frac{m(\utilde{x}| M_2)}{m(\utilde{x}| M_1)}$, por lo que la expresión anterior quedaría, dividiendo todo entre el numerador como: $P(M_1 | \utilde{x}) = \frac{1}{1 + B_21(\utilde{x}) * \frac{\pi(M_2)}{\pi(M_1)}}$.

Por tanto, el cociente entre las probabilidades de que se da cada modelo condicionado a la muestra es:
$  \frac{P(M_2 | \utilde{x})}{P(M_1 | \utilde{x})} = B_{21}(\utilde{x}) * \frac{\pi(M_2)}{\pi(M_1)}$.

\subsection{Método de Leamer}



	En la sección de estadística bayesiana vimos que podíamos escoger distribuciones a priori no integrables (impropias) que aun así nos permitían obtener buenas distribuciones a posteriori sobre las que realizar inferencia. ¿Qué ocurre si cogemos como distribución a priori una distribución impropia? La respuesta se basa en que al calcular el factor de Bayes de una muestra, dicho factor está indeterminado por una constante multiplicativa que proviene de las dos distribuciones a priori impropias, esto es, si $\pi(\theta_i|M_i) = c_ih_i(\theta_i)$, con $h_i$ no integrable y $c_i > 0$ arbitrario, entonces $\pi(\theta_1 | M_1) = c_1 h_1(\theta_1) $ y lo mismo para la el segundo modelo, obtenemos que  $B_{21}(\utilde{x}) = \frac{c_2 \int{f(\utilde{x} | \theta_2 , M_2)h_2(\theta_2) d\theta_2}}{c_1 \int{f(\utilde{x} | \theta_1 , M_1) h_1(\theta_1) d\theta_1}}$. Por ello, debemos coger distribuciones propias.

	El método de Leamer consiste en obtener una submuestra de una muestra, que llamaremos muestra de entrenamiento, de forma que al calcular la distribución a posteriori esté bien definida. Para ello, tomamos $\utilde{x_1}$ submuestra de $\utilde{x}$. Para que $\pi(\theta_1 | \utilde{x_1}, M_1)$ esté bien definida, se tiene que verificar que $ 0 < m(\utilde{x_1} | M_1) < \infty$ (análogo para el modelo $M_2$).

	\begin{definition}
		Dado un modelo $M$ y una muestra $\utilde{x}$, una muestra de entrenamiento es un subconjunto de la muestra $\utilde{x_1} \subset \utilde{x}$. Se dice que es propio si $0 < m(\utilde{x_1}|M) < \infty$.

		Una muestra de entrenamiento $\utilde{x_1}$ se dice que es minimal si es propia y ningún subconjunto suyo distinto de $\utilde{x_1}$ lo es.
	\end{definition}

	Como acabamos de ver, la selección de una muestra de entrenamiento minimal nos permite poder continuar con distribuciones a priori impropias al inicio, que pasarán a ser propias tras entrenarlas con dicha muestra. El principal inconveniente de este método es determinar qué datos son los más convenientes para entrenar la distribución a priori, es decir, qué subconjunto $\utilde{x_1}$ minimal escoger.

	\begin{ex}
		Vimos (Ejemplo \ref{ex:jeff:poisson}) que la distribución de Jeffreys para una Poisson de parámetro $\lambda$ es $\pi^{\mathcal{J}}(\lambda)=c\lambda^{-1/2}$ impropia. Sin embargo, para cualquier muestra $\utilde{x}=(x_1,\dots,x_n)$, teníamos que $m(\utilde{x}) = \frac{c}{n ^{\sum{x_i} + \frac{1}{2}}\prod{x_i!}}\Gamma(\sum{x_i} + \frac{1}{2}) < \infty$. Por tanto, $\utilde{x}$ es una muestra de entrenamiento propia para cualquier tamaño, y en particular lo es para un solo dato. Por tanto, solo necesitamos un dato para hacer la distribución a priori propia, y la muestra de entrenamiento minimal consta de un solo dato.
	\end{ex}

	\begin{ex}
		En una distribución normal $\mathcal{N}(\mu,\sigma^2)$ la distribución de Jeffreys es $\pi^{\mathcal{J}}(\mu,\sigma^2) = \frac{c}{\sigma^2}\chi_{\mathbb{R}\times{\mathbb{R}^{+}}}(\mu,\sigma^2)$, no integrable. Para un solo dato la distribución a posteriori sigue sin ser integrable, pero con dos ya sí lo es. Por tanto, la muestra de entrenamiento minimal es de tamaño 2.
	\end{ex}


\pagebreak
\begin{thebibliography}{99}
\bibitem{loeve} Probability theory, M. Loève, 1977, Springer-Verlag.
\bibitem{casella} Statistical Inference, G. Casella, R. L. Berger, segunda edicón (2002), Duxbury Advanced Series.
\bibitem{gamma} Proof Wiki, Euler's Reflection Formula, \url{https://proofwiki.org/wiki/Euler%27s_Reflection_Formula}.
\bibitem{cauchy} Wikipedia, Residue theorem, \url{https://en.wikipedia.org/wiki/Residue_theorem#Example}.
\bibitem{leibniz} Wikipedia, Leibniz integral rule, \url{https://en.wikipedia.org/wiki/Leibniz_integral_rule}.
\bibitem{char} Davide Giraudo, No first moment and differentiable characteristic function, \url{http://math.stackexchange.com/questions/793788/continuous-probability-distribution-with-no-first-moment-but-the-characteristic}
\end{thebibliography}

\end{document}
